{"title":"Building A Conversational N.L.P Enabled Chatbot Using Google’s Dialogflow","link":"https://smashingmagazine.com/2020/12/conversational-nlp-enabled-chatbot-google-dialogflow/","date":1607439600000,"content":"<div id=\"readability-page-1\" class=\"page\"><div id=\"article__content\"><div><div><figure role=\"presentation\"><a href=\"https://www.smashingmagazine.com/author/nwani-victory\"><p><img src=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/963ff8aa-553c-47c5-bc67-df09adb4bd28/nwani-victory-200x200.jpeg\" loading=\"eager\" width=\"100\" height=\"100\" alt=\"Nwani Victory\"></p></a></figure></div><div id=\"author__desc\"><p>Nwani Victory works remotely as a Fullstack developer from Lagos, Nigeria. After office hours, he doubles as a Cloud Engineer seeking ways to make Cloud … <a href=\"https://www.smashingmagazine.com/author/nwani-victory\">More about Nwani ↬</a></p></div></div><p><section aria-label=\"quick summary\">The 2019 <a href=\"https://www.capgemini.com/research-institute/\">Capgemini research institute</a>’s <a href=\"https://www.capgemini.com/wp-content/uploads/2019/09/Report_Conversational-Interfaces-1.pdf\">report</a> published after a research on the use of chat assistants showed a drastic 76% increase in customer satisfaction from organizations where chat assistants where built and incorporated into their services. But how does <a href=\"https://cloud.google.com/dialogflow/docs\">Dialogflow</a>, a product from Google’s ecosystem, aid developers in building chat assistants and contribute to this quota?</section></p><p>Ever since <a href=\"https://en.wikipedia.org/wiki/ELIZA\">ELIZA</a> (the first <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">Natural Language Processing</a> computer program brought to life by <a href=\"https://en.wikipedia.org/wiki/Joseph_Weizenbaum\">Joseph Weizenbaum</a> in 1964) was created in order to process user inputs and engage in further discussions based on the previous sentences, there has been an increased use of <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">Natural Language Processing</a> to extract key data from human interactions. One key application of Natural language processing has been in the creation of conversational chat assistants and voice assistants which are used in mobile and web applications to act as customer care agents attending to the virtual needs of customers.</p><p>In 2019, the <a href=\"https://www.capgemini.com/research-institute/\">Capgemini Research Institute</a> released a report after conducting a survey on the impact which chat assistants had on users after being incorporated by organizations within their services. The key findings from this survey showed that many customers were highly satisfied with the level of engagement they got from these chat assistants and that the number of users who were embracing the use of these assistants was fast growing!</p><p>To quickly build a chat assistant, developers and organizations leverage SaaS products running on the cloud such as <a href=\"https://cloud.google.com/dialogflow/docs\">Dialogflow</a> from Google, <a href=\"https://www.ibm.com/cloud/watson-conversation\">Watson Assistant</a> from IBM, <a href=\"https://azure.microsoft.com/en-us/services/bot-service/#overview\">Azure Bot Service</a> from Microsoft, and also <a href=\"https://aws.amazon.com/lex/\">Lex</a> from Amazon to design the chat flow and then integrate the natural language processing enabled chat-bots offered from these services into their own service.</p><p>This article would be beneficial to developers interested in building conversational chat assistants using <a href=\"https://cloud.google.com/dialogflow/docs\">Dialogflow</a> as it focuses on the <a href=\"https://cloud.google.com/dialogflow/docs\">Dialogflow</a> itself as a Service and how chat assistants can be built using the <a href=\"https://cloud.google.com/dialogflow/es/docs/console\">Dialogflow console</a>.</p><p><strong>Note</strong>: <em>Although the <a href=\"https://cloud.google.com/dialogflow/es/docs/fulfillment-webhook\">custom webhooks</a> built within this article are well explained, a fair understanding of the JavaScript language is required as the webhooks were written using JavaScript.</em></p><h3 id=\"dialogflow\">Dialogflow</h3><p>Dialogflow is a platform that simplifies the process of creating and designing a natural language processing conversational chat assistant which can accept voice or text data when being used either from the Dialogflow console or from an integrated web application.</p><p>To understand how Dialogflow simplifies the creation of a conversational chat assistant, we will use it to build a customer care agent for a food delivery service and see how the built chat assistant can be used to handle food orders and other requests of the service users.</p><p>Before we begin building, we need to understand some of the key terminologies used on Dialogflow. One of Dialogflow’s aim is to abstract away the complexities of building a Natural Language Processing application and provide a console where users can visually create, design, and train an AI-powered chatbot.</p><h3 id=\"dialog-flow-terminologies\">Dialog Flow Terminologies</h3><p>Here is a list of the Dialogflow terminologies we will consider in this article in the following order:</p><ul><li><p><strong>Agent</strong><br>An agent on Dialogflow represents the chatbot created by a user to interact with other end-users and perform data processing operations on the information it receives. Other components come together to form an agent and each time one of these components is updated, the agent is immediately re-trained for the changes to take effect.</p><p>User’s who want to create a full-fledged conversational chatbot within the quickest time possible can select an agent from the <a href=\"https://cloud.google.com/dialogflow/es/docs/agents-prebuilt\">prebuilt agents</a> which can be likened to a template which contains the basic intents and responses needed for a <em>conversational assistant</em>.</p><p><strong>Note:</strong> <em>A conversational assistant on Dialogflow will now be referred to as an “agent” while someone else asides the author of the assistant who interacts with it would be referred to as an “end-user”.</em></p></li><li><p><strong>Intent</strong><br>Similar to its literal meaning, the intent is the user’s end goal in each sentence when interacting with an agent. For a single agent, multiple intents can be created to handle each sentence within a conversation and they are connected together using Contexts.</p><p>From the intent, an agent is able to understand the end-goal of a sentence. For example, an agent created to process food orders from customers would be to recognize the end-goal of a customer to place an order for a meal or get recommendations on the available meals from a menu using the created intents.</p></li><li><p><strong>Entity</strong><br>Entities are a means by which Dialogflow processes and extracts specific data from an end-user’s input. An example of this is a Car entity added to an intent. Names of vehicles would be extracted from each sentence input as the Car entity.</p><p>By default, an agent has some System entities which have predefined upon its creation. Dialogflow also has the option to define custom entities and add values recognizable within this entity.</p></li><li><p><strong>Training Phrase</strong><br>The training phrases is a major way in which an agent is able to recognize the intent of an end-user interacting with the agent. Having a large number of training phrases within an intent increases the accuracy of the agent to recognize an intent, in fact <a href=\"https://cloud.google.com/dialogflow/es/docs/intents-training-phrases\">Dialogflow’s documentation</a> on training phases recommends that “at least 10-20” training phrases be added to a created intent.</p><p>To make training phrases more reusable, dialogflow gives the ability to annotate specific words within the training phrase. When a word within a phrase is annotated, dialogflow would recognize it as a placeholder for values that would be provided in an end-user’s input.</p></li><li><p><strong>Context</strong><br>Contexts are string names and they are used to control the flow of a conversation with an agent. On each intent, we can add multiple input contexts and also multiple output contexts. When the end-user makes a sentence that is recognized by an intent the output contexts become active and one of them is used to match the next intent.</p><p>To understand contexts better, we can illustrate context as the security entry and exit door, while the intent as the building. The input context is used when coming into the building and it accepts visitors that have been listed in the intent while the exit door is what connects the visitors to another building which is another intent.</p></li><li><p><strong>Knowledge base</strong><br>A knowledge base represents a large pool of information where an agent can fetch data when responding to an intent. This could be a document in any format such as <code>txt</code>, <code>pdf</code>, <code>csv</code> among other <a href=\"https://cloud.google.com/dialogflow/es/docs/how/knowledge-bases#supported-content\">supported</a> document types. In machine learning, a knowledge base could be referred to as a <strong>training dataset</strong>.</p><p>An example scenario where an agent might refer to a knowledge base would be where an agent is being used to find out more details about a service or business. In this scenario, an agent can refer to the service’s Frequently Asked Questions as its knowledge base.</p></li><li><p><strong>Fulfillment</strong><br>Dialogflow’s Fulfillment enables an agent to give a more dynamic response to a recognized intent rather than a static created response. This could be by calling a defined service to perform an action such as creating or retrieving data from a database.</p><p>An intent’s fulfillment is achieved through the use of a webhook. Once enabled, a matched intent would make an API request to the webhook configured for the dialogflow agent.</p></li></ul><p>Now, that we have an understanding of the terminologies used with Dialogflow, we can move ahead to use the <a href=\"https://dialogflow.cloud.google.com/cx/projects\">Dialogflow console</a> to create and train our first agent for a hypothetical food service.</p><h3 id=\"using-the-dialogflow-console\">Using The Dialogflow Console</h3><p><strong>Note:</strong> <em>Using the Dialogflow console requires that a Google account and a project on the Google Cloud Platform is created. If unavailable, a user would be prompted to sign in and create a project on first use.</em></p><p>The Dialogflow console is where the agent is created, designed, and trained before integrating with other services. Dialogflow also provides <a href=\"https://cloud.google.com/dialogflow/es/docs/reference/rest/v2-overview\">REST API endpoints</a> for users who do not want to make use of the console when building with Dialogflow.</p><p>While we go through the console, we will gradually build out the agent which would act as a customer care agent for a food delivery service having the ability to list available meals, accept a new order and give information about a requested meal.</p><p>The agent we’ll be building will have the conversation flow shown in the flow chart diagram below where a user can purchase a meal or get the list of available meals and then purchase one of the meals shown.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png\" sizes=\"100vw\" alt=\"A diagram of the conversation flow of the proposed agent to be built.\"></a><figcaption>A diagram of the conversation flow of the proposed agent to be built. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/37cbc957-eddb-465c-9530-88db43b0749f/dialogflow-demo-agent-flowchart.png\">Large preview</a>)</figcaption></figure><h4 id=\"creating-a-new-agent\">Creating A New Agent</h4><p>Within every newly created project, Dialogflow would prompt the first time user to create an agent which takes the following fields:</p><ul><li>A name to identify the agent.</li><li>A language which the agent would respond in. If not provided the default of English is used.</li><li>A project on the Google Cloud to associate the agent with.</li></ul><p>Immediately after we click on the create button after adding the values of the fields above, a new agent would be saved and the intents tab would be shown with the Default fallback and Default Welcome intent as the only two available intents which are created by default with every agent on Dialogflow.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png\" sizes=\"100vw\" alt=\"The intents tab with the two default created intents\"></a><figcaption>The intents tab with the two default created intents. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/932899bb-3e65-4aae-adea-7f0a7202d94e/intent-list-tab.png\">Large preview</a>)</figcaption></figure><p>Exploring the Default fallback intent, we can see it has no training phrase but has sentences such as “Sorry, could you say that again?”, “What was that?”, “Say that one more time?” as responses to indicate that the agent was not able to recognize a sentence which has been made by an end-user. During all conversations with the agent, these responses are only used when the agent cannot recognize a sentence typed or spoken by a user.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png\" sizes=\"100vw\" alt=\"The Default Fallback intent page with the responses listed out.\"></a><figcaption>The Default Fallback intent page with the responses listed out. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/165a9cdc-2a57-44a2-b2c6-18b51b792488/default-fallback-intent-responses.png\">Large preview</a>)</figcaption></figure><p>While the sentences above are sufficient for indicating that agent does not understand the last typed sentence, we would like to aid the end-user by giving them some more information to hint the user on what the agent can recognize. To do this, we replace all the listed sentences above with the following ones and click the Save button for the agent to be retrained.</p><p>From each of the four sentences above, we see can observe that the agent could not recognize what the last sentence made was and also a piece of information on what the agent can do thus hinting the user on what to type next in order to continue the conversation.</p><p>Moving next to the <strong>Default Welcome Intent</strong>, the first section on the intent page is the <a href=\"https://cloud.google.com/dialogflow/es/docs/contexts-overview\">Context section</a> and expanding it we can see both the input and output contexts are blank. From the conversation flow of the agent shown previously, we want an end-user to either place a meal order or request a list of all available meals. This would require the two following new output contexts they would each become active when this intent is matched;</p><ul><li><p><code>awaiting_order_request</code><br>This would be used to match the intent handling order requests when an end-user wants to place an order for a meal.</p></li><li><p><code>awaiting_info_request</code><br>This would be used to match the intent that retrieves data of all the meals when an end-user wants to know the available meals.</p></li></ul><p>After the context section is the intent’s <strong>Events</strong> and we can see it has the <code>Welcome</code> event type added to the list of events indicating that this intent will be used first when the agent is loaded.</p><p>Coming next are the <strong>Training Phrases</strong> for the intent. Due to being created by default, it already has 16 phrases that an end-user would likely type or say when they interact with the agent for the first time.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png\" sizes=\"100vw\" alt=\"The Default Fallback intent page with the default added Training phrases listed\"></a><figcaption>The Default Fallback intent page with the default added Training phrases listed. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/47d99880-134a-4e7c-aa11-7fbfcd1a4902/default-welcome-training-phrases.png\">Large preview</a>)</figcaption></figure><p>When an end-user types or makes a sentence similar to those listed in the training phrases above, the agent would respond using a picked response from the Responses list section shown below:</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png\" sizes=\"100vw\" alt=\"A list generated responses within the Default Welcome intent.\"></a><figcaption>A list generated responses within the Default Welcome intent. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/14e00981-7f88-44ee-9df3-64348790d54d/default-welcome-intent-text-responses.png\">Large preview</a>)</figcaption></figure><p>Each of the responses above is automatically generated for every agent on Dialogflow. Although they are grammatically correct, we would not use them for our food agent. Being a default intent that welcomes an end-user to our agent, a response from the agent should tell what organization it belongs to and also list its functionalities in a single sentence.</p><p>We would delete all the responses above and replace them with the ones below to better help inform an end-user on what to do next with the agent.</p><p>From the two responses above, we can see it tells an end-user what the name of the bot is, the two things the agent can do, and lastly, it pokes the end-user to take further action. Taking further action further from this intent means we need to connect the Default Welcome Intent to another. This is possible on Dialogflow using context.</p><p>When we add and save those two phrases above, dialogflow would immediately re-train the agent so I can respond using any one of them.</p><p>Next, we move on to create two more intents to handle the functionalities which we have added in the two responses above. One to purchase a food item and the second to get more information about meals from our food service.</p><h4 id=\"creating-list-meals-intent\">Creating list-meals intent:</h4><p>Clicking the + ( add ) icon from the left navigation menu would navigate to the page for creating new intents and we name this intent <strong>list-available-meals</strong>.</p><p>From there we add an output context with the name <code>awaiting-order-request</code>. This output context would be used to link this intent to the next one where they order a meal as we expect an end-user to place an order for a meal after getting the list of meals available.</p><p>Moving on to the Training Phrases section on the intent page, we will add the following phrases provided by the end-user in order to find out which meals are available.</p><p>Next, we would add just the single fallback response below to the Responses section;</p><p>From the response above we can observe that it indicates that the meal’s list is unavailable or an error has occurred somewhere. This is because it is a fallback response and would only be used when an error occurs in fetching the meals. The main response would come as a fulfillment using the webhooks option which we will set up next.</p><p>The last section in this intent page is the <strong>Fulfillment</strong> section and it is used to provide data to the agent to be used as a response from an externally deployed API or source. To use it we would enable the <strong>Webhook call</strong> option in the Fulfillment section and set up the fulfillment for this agent from the fulfillment tab.</p><h4 id=\"managing-fulfillment\">Managing Fulfillment:</h4><p>From the Fulfillment tab on the console, a developer has the option of using a webhook which gives the ability to use any deployed API through its endpoint or use the Inline Code editor to create a serverless application to be deployed as a cloud function on the Google Cloud. If you would like to know more about serverless applications, <a href=\"https://www.smashingmagazine.com/2020/11/serverless-frontend-applications-google-cloud-platform/\">this article</a> provides an excellent guide on getting started with serverless applications.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png\" sizes=\"100vw\" alt=\"The fulfillment tab for a created agent on Dialogflow.\"></a><figcaption>The fulfillment tab for a created agent on Dialogflow. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8b41fc7c-c153-47b8-8675-a718fd6c3bb9/agent-fulfillment.png\">Large preview</a>)</figcaption></figure><p>Each time an end-user interacts with the agent and the intent is matched, a <a href=\"https://en.wikipedia.org/wiki/POST_(HTTP)\">POST</a> request would be made to the endpoint. Among the various object fields in the <a href=\"https://cloud.google.com/dialogflow/es/docs/fulfillment-webhook#webhook_request\">request body</a>, only one is of concern to us, i.e. the <code>queryResult</code> object as shown below:</p><p>While there are other fields in the <code>queryResult</code> such as a context, the parameters object is more important to us as it holds the parameter extracted from the user’s text. This parameter would be the meal a user is requesting for and we would use it to query the food delivery service database.</p><p>When we are done setting up the fulfillment, our agent would have the following structure and flow of data to it:</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png\" sizes=\"100vw\" alt=\"The diagram showing the flow for the food delivery agent.\"></a><figcaption>The diagram showing the flow for the food delivery agent. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f7c61506-7e79-410a-8b57-6bf3247a8245/dialogflow-delivery-dataflow.png\">Large preview</a>)</figcaption></figure><p>From the diagram above, we can observe that the cloud function acts as a middleman in the entire structure. The Dialogflow agent sends the parameter extracted from an end user’s text to the cloud function in a <a href=\"https://cloud.google.com/dialogflow/es/docs/fulfillment-webhook#webhook_request\">request payload</a> and the cloud function, in turn, queries the database for the document using the received name and sends back the queried data in a response payload to the agent.</p><p>To start an implementation of the design system above, we would begin with creating the cloud function locally in a development machine then connect it to our dialogflow agent using the custom webhook option. After it has been tested, we can switch to using the inline editor in the fulfillment tab to create and deploy a cloud function to work with it. We begin this process by running the following commands from the command line:</p><p>After installing the needed packages, we modify the generated <code>package.json</code> file to include two new objects which enable us to run a cloud function locally using the <a href=\"https://cloud.google.com/functions/docs/functions-framework\">Functions Framework</a>.</p><p>The start command in the scripts above tells the functions Framework to run the <code>foodFunction</code> in the <code>index.js</code> file and also makes it listen and serve connections through our localhost on port <code>8000</code>.</p><p>Next is the content of the index.js file which holds the function; we’ll make use of the code below since it connects to a MongoDB database and queries the data using the parameter passed in by the Dialogflow agent.</p><p>From the code snippet above we can see that our cloud function is pulling data from a MongoDB database, but let’s gradually step through the operations involved in pulling and returning this data.</p><ul><li><p>First, the cloud function initiates a connection to a <a href=\"https://www.mongodb.com/cloud/atlas\">MongoDB Atlas cluster</a>, then it opens the collection storing the meal category documents within the database being used for the food-service on the cluster.</p></li><li><p>Next, using the parameter passed into the request from the user’s input, we run a <a href=\"https://docs.mongodb.com/manual/reference/method/db.collection.find/\">find</a> method on the collection to get which then returns a <a href=\"https://docs.mongodb.com/manual/reference/method/js-cursor/\">cursor</a> which we further iterate upon to get all the MongoDB documents within the collection containing the data.</p></li><li><p>We model the data returned from MongoDB into Dialogflow’s <a href=\"https://cloud.google.com/dialogflow/es/docs/intents-rich-messages#card\">Rich response message</a> object structure which displays each of the meal items to the end-user as a card with an image, title, and a description.</p></li><li><p>Finally, we send back the entire data to the agent after the iteration in a JSON body and end the function’s execution with a <code>200</code> status code.</p></li></ul><p><strong>Note:</strong> The Dialogflow agent would wait for a response after a request has been sent within a frame of 5 seconds. This waiting period is when the loading indicator is shown on the console and after it elapses without getting a response from the webhook, the agent would default to using one of the responses added in the intent page and return a <code>DEADLINE EXCEEDED</code> error. This limitation is worth taking note of when designing the operations to be executed from a webhook. The <a href=\"https://cloud.google.com/dialogflow/es/docs/best-practices#retries\">API error retries</a> section within the Dialogflow <a href=\"https://cloud.google.com/dialogflow/es/docs/best-practices\">best practices</a> contains steps on how to implement a retry system.</p><p>Now, the last thing needed is a <code>.env</code> file created in the project directory with the following fields to store the environment variables used in the <code>index.js</code>.</p><p>At this point, we can start the function locally by running <code>yarn start</code> from the command line in the project’s directory. For now, we still cannot make use of the running function as Dialogflow only supports secure connections with an SSL certificate, and where <a href=\"https://ngrok.com/\">Ngrok</a> comes into the picture.</p><p>Using <a href=\"https://ngrok.com/\">Ngrok</a>, we can create a tunnel to expose the localhost port running the cloud function to the internet with an SSL certificate attached to the secured connection using the command below from a new terminal;</p><p>This would start the tunnel and generate a forwarding URL which would be used as an endpoint to the function running on a local machine.</p><p><strong>Note:</strong> <em>The extra <code>-bind-tls=true</code> argument is what instructs <a href=\"https://ngrok.com/\">Ngrok</a> to create a secured tunnel rather than the unsecured connection which it creates by default.</em></p><p>Now, we can copy the URL string opposite the forwarding text in the terminal and paste in the URL input field which is found in the Webhook section, and then save it.</p><p>To test all that has been done so far, we would make a sentence to the Dialogflow agent requesting the list of meals available using the Input field at the top right section in the Dialogflow console and watch how it waits for and uses a response sent from the running function.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png\" sizes=\"100vw\" alt=\"A test of the created list-meals intent and its returned data result.\"></a><figcaption>A test of the created list-meals intent and its returned data result. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0741fae9-f597-4343-b334-414fc8a38dcc/list-meals-intent-demo-test.png\">Large preview</a>)</figcaption></figure><p>Starting from the center placed terminal in the image above, we can the series of <code>POST</code> requests made to the function running locally and on the right-hand side the data response from the function formatted into cards.</p><p>If for any reason a webhook request becomes unsuccessful, Dialogflow would resolve the error by using one of the listed responses. However, we can find out why the request failed by using the Diagnostic Info tool updated in each conversation. Within it are the <strong>Raw API response</strong>, <strong>Fulfillment request</strong>, <strong>Fulfillment response</strong>, and <strong>Fulfillment status</strong> tabs containing <code>JSON</code> formatted data. Selecting the Fulfillment response tab we can see the response from the webhook which is the cloud function running on our local machine.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png\" sizes=\"100vw\" alt=\"Diagnostics info modal with the Fulfillment response tab active.\"></a><figcaption>The Diagnostics info modal with the Fulfillment response tab active showing the webhook response in JSON format. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/ada0462e-62cc-4868-b3bc-34f0ec6ae065/diagnostic-info.png\">Large preview</a>)</figcaption></figure><p>At this point, we expect a user to continue the conversation with an order of one of the listed meals. We create the last intent for this demo next to handle meal orders.</p><h3 id=\"creating-request-meal-intent\">Creating Request-meal Intent:</h3><p>Following the same steps used while creating the first intent, we create a new intent using the console and name it <code>request-meal</code> and add an input context of <code>awaiting_order_request</code> to connect this intent from either the Default Welcome intent or the list-available meals intent.</p><p>Within the training phrase section, we make use of the following phrases,</p><p>Reading through the phrases above, we can observe they all indicate one thing — the user wants food. In all of the phrases listed above, the name or type of food is not specified but rather they are all specified as <strong>food</strong>. This is because we want the food to be dynamic value, if we were to list all the food names we certainly would need to have a very large list of training phrases. This also applies to the amount and price of the food being ordered, they would be annotated and the agent would be able to recognize them as a placeholder for the actual values within an input.</p><p>To make a value within a phrase dynamic, dialogflow provides entities. Entities represent common types of data, and in this intent, we use entities to match several food types, various price amounts, and quantity from an end user’s sentence to request.</p><p>From the training phrases above, dialogflow would recognize <strong>$40</strong> as <code>@sys.unit-currency</code> which is under the amounts-with-units category of the <a href=\"https://cloud.google.com/dialogflow/es/docs/entities-system\">system entities list</a> and <strong>2</strong> as <code>@number</code> under the number category of the <a href=\"https://cloud.google.com/dialogflow/es/docs/entities-system\">system entities list</a>. However, <code>food</code> is not a not a recognized <a href=\"https://cloud.google.com/dialogflow/es/docs/entities-system\">system entity</a>. In a case such as this, dialogflow gives developers the option to create a <a href=\"https://cloud.google.com/dialogflow/docs/entities-custom\">custom entity</a> to be used.</p><h4 id=\"managing-entities\">Managing Entities</h4><p>Double-clicking on <code>food</code> would pop up the entities dropdown menu, at the bottom of the items in the dropdown we would find the <strong>Create new entity</strong> button, and clicking it would navigate to the Entities tab on the dialogflow console, where we can manage all entities for the agent.</p><p>When at the entities tab, we name this new entity as <code>food</code> then at the options dropdown located at the top navigation bar beside the <strong>Save</strong> button we have the option to switch the entities input to a raw edit mode. Doing this would enable us to add several entity values in either a <a href=\"https://en.wikipedia.org/wiki/JSON\">json</a> or <a href=\"https://en.wikipedia.org/wiki/CSV\">csv</a> format rather than having to add the entities value one after the other.</p><p>After the edit mode has been changed, we would copy the sample <a href=\"https://en.wikipedia.org/wiki/JSON\">JSON</a> data below into the editor box.</p><p>From the <a href=\"https://en.wikipedia.org/wiki/JSON\">JSON</a> formatted data above, we have 15 meal examples. Each object in the array has a <strong>“value”</strong> key which is the name of the meal and a <strong>“synonyms”</strong> key containing an array of names very similar to the object’s value.</p><p>After pasting the <a href=\"https://en.wikipedia.org/wiki/JSON\">json</a> data above, we also check the <strong>Fuzzy Matching</strong> checkbox as it enables the agent to recognize the annotated value in the intent even when incompletely or slightly misspelled from the end user’s text.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png\" sizes=\"100vw\" alt=\"JSON data values added to the newly created food entity in raw editor mode.\"></a><figcaption>JSON data values added to the newly created food entity in raw editor mode. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f07fe319-20e1-4132-851a-05b9b4e13811/food-entity-values.png\">Large preview</a>)</figcaption></figure><p>After saving the entity values above, the agent would immediately be re-trained using the new values added here and once the training is completed, we can test by typing a text in the input field at the right section.</p><p>Responses within this intent would be gotten from our previously created function using the intent’s fulfillment webhook, however, we add the following response to serve as a fallback to be used whenever the webhook is not executed successfully.</p><p>We would also modify the code of the existing cloud function to fetch a single requested as it now handles requests from two intents.</p><p>From the highlighted parts above, we can see the following new use cases that the function has now been modified to handle:</p><ul><li><strong>Multiple intents</strong><br>the cloud function now uses a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/switch\">switch statement</a> with the intent’s name being used as cases. In each <a href=\"https://cloud.google.com/dialogflow/es/docs/fulfillment-webhook#webhook_request\">request payload</a> made to a webhook, Dialogflow includes details about the intent making the request; this is where the intent name is being pulled from to match the cases within the switch statement.</li><li><strong>Fetch a single meal</strong><br>the Meals collection is now queried using the value extracted as a parameter from the user’s input.</li><li>A <strong>call-to-action button</strong> is now being added to the card which a user can use to pay for the requested meal and clicking it opens a tab in the browser. In a functioning chat assistant, this button’s <code>postback</code> URL should point to a checkout page probably using a configured third-party service such as <a href=\"https://stripe.com/payments/checkout\">Stripe checkout</a>.</li></ul><p>To test this function again, we restart the function for the new changes in the <code>index.js</code> file to take effect and run the function again from the terminal by running <code>yarn start</code>.</p><p><strong>Note:</strong> <em>You don’t have to restart the terminal running the <a href=\"https://ngrok.com/\">Ngrok</a> tunnel for the new changes to take place. <a href=\"https://ngrok.com/\">Ngrok</a> would still forward requests to the updated function when the webhook is called.</em></p><p>Making a test sentence to the agent from the dialogflow console to order a specific meal, we can see the <code>request-meal</code> case within the cloud function being used and a single card getting returned as a response to be displayed.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png\" sizes=\"100vw\" alt=\"Testing the request-meal intent through the Dialogflow console emulator.\"></a><figcaption>A meal card from testing the request-meal intent using the Dialogflow console emulator. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/576e080e-8fee-4341-88d3-7ec232503ac0/request-meal-intent-response-test.png\">Large preview</a>)</figcaption></figure><p>At this point, we can be assured that the cloud function works as expected. We can now move forward to deploy the local function to the <a href=\"https://cloud.google.com/functions/docs\">Google Cloud Functions</a> using the following command;</p><p>Using the command above deploys the function to the Google Cloud with the flags explained below attached to it and logs out a generated URL endpoint of deployed cloud function to the terminal.</p><ul><li><p><strong><code>NAME</code></strong><br>This is the name given to a cloud function when deploying it and is it required. In our use case, the name of the cloud function when deployed would be <code>foodFunction</code>.</p></li><li><p><strong><code>trigger-http</code></strong><br>This selects HTTP as the function’s trigger type. Cloud functions with an HTTP trigger would be invoked using their generated URL endpoint. The generated URLs are secured and use the <code>https</code> protocol.</p></li><li><p><strong><code>entry-point</code></strong><br>This the specific exported module to be deployed from the file where the functions were written.</p></li><li><p><strong><code>set-env-vars</code></strong><br>These are the environment variables available to the cloud function at runtime. In our cloud function, we only access our <code>MONGODB_URI</code> and <code>DATABASE_NAME</code> values from the environment variables.</p><p>The MongoDB connection string is gotten from a created MongoDB cluster on <a href=\"https://www.mongodb.com/cloud/atlas\">Atlas</a>. If you need some help on creating a cluster, the MongoDB <a href=\"https://docs.atlas.mongodb.com/getting-started/\">Getting started</a> section provides great help.</p></li><li><p><strong><code>allow-authenticated</code></strong><br>This allows the function to be invoked outside the Google Cloud through the Internet using its generated endpoint without checking if the caller is authenticated.</p></li></ul><h3 id=\"dialogflow-integrations\">Dialogflow Integrations</h3><p>Dialogflow gives developers the feature to integrate a built agent into several conversational platforms including social media platforms such as Facebook Messenger, Slack, and Telegram. Asides from the two integration platforms which we used for our built agent, the <a href=\"https://cloud.google.com/dialogflow/es/docs/integrations\">Dialogflow documentation</a> lists the available types of integrations and platforms within each integration type.</p><h4 id=\"integrating-with-google-actions\">Integrating With Google Actions</h4><p>Being a product from Google’s ecosystem, agents on Dialogflow integrate seamlessly with <a href=\"https://assistant.google.com/\">Google Assistant</a> in very few steps. From the Integrations tab, <a href=\"https://assistant.google.com/\">Google Assistant</a> is displayed as the primary integration option of a dialogflow agent. Clicking the <a href=\"https://assistant.google.com/\">Google Assistant</a> option would open the Assistant modal from which we click on the test app option. From there the <a href=\"https://console.actions.google.com/\">Actions console</a> would be opened with the agent from Dialogflow launched in a test mode for testing using either the voice or text input option.</p><figure><a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png\"><img loading=\"lazy\" decoding=\"async\" importance=\"low\" srcset=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png 400w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png 800w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png 1200w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png 1600w, https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png 2000w\" src=\"https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png\" sizes=\"100vw\" alt=\"Testing the Dialogflow agent from the Google Actions console.\"></a><figcaption>Using Google assistant integration to test the Dialogflow agent from the Google Actions console in a test mode. (<a href=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b0a2d288-0526-482f-ae8c-f86d173071a4/google-action-test-app.png\">Large preview</a>)</figcaption></figure><p>Integrating a dialogflow agent with the Google Assistant is a huge way to make the agent accessible to millions of Google Users from their Smartphones, Watches, Laptops, and several other connected devices. To publish the agent to the Google Assistant, the <a href=\"https://developers.google.com/assistant/console/publish\">developers docs</a> provides a detailed explanation of the process involved in the deployment.</p><h4 id=\"integrating-with-a-web-demo\">Integrating With A Web Demo</h4><p>The <a href=\"https://cloud.google.com/dialogflow/es/docs/integrations/web-demo\">Web Demo</a> which is located in the Text-based sections of the Integrations Tab in the Dialogflow console allows for the use of the built agent in a web application by using it in an <a href=\"https://www.w3schools.com/html/html_iframe.asp\">iframe</a> window. Selecting the web Demo option would generate a URL to a page with a chat window that simulates a real-world chat application.</p><p><strong>Note:</strong> <em>Dialogflow’s web demo only supports text responses and does not support the display of <a href=\"https://cloud.google.com/dialogflow/es/docs/intents-rich-messages#card\">Rich messages</a> and images. This worth noting when using a webhook that responds with data in the <a href=\"https://cloud.google.com/dialogflow/es/docs/intents-rich-messages#card\">Rich response</a> format.</em></p><h3 id=\"conclusion\">Conclusion</h3><p>From several surveys, we can see the effect of chat assistants on customer satisfaction when incorporated by organizations into their services. These positive metrics are expected to grow up in the next coming years thus placing greater importance on the use of these chat assistants.</p><p>In this article, we have learned about Dialogflow and how it is providing a platform for organizations and developers to build Natural Language processing conversational chat assistants for use in their services. We also moved further to learn about its terminologies and how these terminologies apply when building a chat assistant by building a demo chat assistant using the Dialogflow console.</p><p>If a chat assistant is being built to be used at a production level, it is highly recommended that the developer(s) go through the <a href=\"https://cloud.google.com/dialogflow/es/docs/best-practices\">Dialogflow best practices</a> section of the <a href=\"https://cloud.google.com/dialogflow/docs\">documentation</a> as it contains standard design guidelines and solutions to common pitfalls encountered while building a chat assistant.</p><p>The source code to the JavaScript webhook built within this article has been pushed to GitHub and can be accessed from this <a href=\"https://github.com/vickywane/dialogflow-article-server\">repository</a>.</p><h4 id=\"references\">References</h4><ul><li><a href=\"https://cloud.google.com/dialogflow/docs\">Dialogflow</a></li><li><a href=\"https://ngrok.com/\">https://ngrok.com/</a></li><li><a href=\"https://www.mongodb.com/\">https://www.mongodb.com/</a></li><li><a href=\"https://cloud.google.com/functions/docs\">Cloud Functions documentation</a></li><li><a href=\"https://www.smashingmagazine.com/2020/11/serverless-frontend-applications-google-cloud-platform/\">Building Serverless Front-End Applications Using Google Cloud Platform</a> by Nwani Victory</li><li><a href=\"https://developers.google.com/assistant/console/publish\">Actions console</a></li></ul><p><img src=\"https://www.smashingmagazine.com/images/logo/logo--red.png\" alt=\"Smashing Editorial\" width=\"35\" height=\"46\" loading=\"lazy\" decoding=\"async\"> <span>(ks, ra, yk, il)</span></p></div></div>","author":"","siteTitle":"Articles on Smashing Magazine — For Web Designers And Developers","siteHash":"ab069ca35bf300e9db0da36f49701f66485a5b0d2db0471dfeee07cef6204939","entryHash":"7061c0ce134788ff4dae75f324861277871184fcca441ea8cfe923dceb31f86b","category":"Tech"}