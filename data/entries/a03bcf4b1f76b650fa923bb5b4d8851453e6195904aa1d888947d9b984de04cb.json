{"title":"Gating Access to Kubernetes API & Workloads with HashiCorp Boundary","link":"https://www.hashicorp.com/blog/gating-access-to-kubernetes-with-hashicorp-boundary","date":1612378800000,"content":"<p>In this post, we’ll look at the latest HashiCorp Boundary and Kubernetes integration and the access problems it solves. Kubernetes gives DevOps teams a platform to abstract container orchestration and manage the lifecycle of applications at scale in organizations of all sizes. HashiCorp has made a concerted effort to support and develop native integrations with Kubernetes across our projects.</p>\n<p>HashiCorp Boundary is an open source project that enables practitioners and operators to securely access dynamic hosts and services with fine-grained authorization without requiring direct network access and was launched at HashiConf in 2020. In the four months since its public debut, the Boundary engineering team has been working hard to develop core maturity as well as ecosystem integrations.</p>\n<h2><a class=\"__permalink-h\" href=\"#kubernetes-access-model\" aria-label=\"kubernetes access model permalink\">»</a><a class=\"__target-h\" id=\"kubernetes-access-model\" aria-hidden></a>Kubernetes Access Model</h2>\n<p>Today, the <a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/\">access model</a> for applications running on Kubernetes is a coarse-grained experience. Kubernetes users can fork a shell within a container using <code>kubectl exec</code>. This model solves access to the container, but once the user gains access they have a blank check to do as they wish inside that container. Other controls are necessary to safeguard the container runtime as well as other containers running on that Kubernetes cluster.</p>\n<p>Securing containers is a multi-faceted process that often transverses the technology stack, it starts with host-level security, runs up the stack to container <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/\">platform security</a>, how the container runtime is forked, then gets into Linux primitives such as how users and groups are managed within the container image itself. It all goes without saying: container security is hard.</p>\n<p>The extra controls needed around the container runtime itself are a byproduct of <code>kubectl exec</code> giving shell access to a container. Yes, there are <a href=\"https://blog.jessfraz.com/post/containers-security-and-echo-chambers/\">many methods</a> for accessing applications running inside a container on Kubernetes. None of these access models necessitate forking a shell as they rely on ingress controllers or other network abstractions for giving a user access to a TCP session. However, if you want to protect access with role or attribute-based access control (RBAC or ABAC), it can be a <a href=\"https://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes/\">complicated process</a> when you want to integrate these access models with existing solutions you may have such as a company directory.</p>\n<h2><a class=\"__permalink-h\" href=\"#gating-kubernetes-with-boundary\" aria-label=\"gating kubernetes with boundary permalink\">»</a><a class=\"__target-h\" id=\"gating-kubernetes-with-boundary\" aria-hidden></a>Gating Kubernetes with Boundary</h2>\n<p>This is where HashiCorp Boundary comes in. By running Boundary on Kubernetes, you can restrict network access for ingress to one point, the Boundary pod. This ingress point for users can proxy all necessary TCP sessions to any container on Kubernetes. This can be done with minimal operational overhead and next to no YAML engineering.</p>\n<p>The outcome of this is an access model capable of seamless integration with a fully functional RBAC system. What is even more compelling is that this same tool can integrate with an underlying cloud platform to provide one tool to access the infrastructure for the underlying cluster, to the workloads running on top. Imagine being able to start a fully managed, RBAC-controlled SSH session to a VM underpinning your Kubernetes cluster, and then starting another session with the same tool to get access to the application running in a container on top of that cluster. That’s the power of HashiCorp Boundary on Kubernetes.</p>\n<p>Gating access to services within your cluster with Boundary is only the beginning. In our latest <a href=\"https://www.boundaryproject.io/downloads\">0.1.4 release</a>, we have included a new <code>boundary connect kube</code> feature. This command forks <code>kubectl</code> under the hood to allow you to gate API calls to your Kubernetes cluster with Boundary. This allows you to use the same tool across your development and operations teams to administer Kubernetes and access workloads on top of it.</p>\n<h2><a class=\"__permalink-h\" href=\"#what-s-next\" aria-label=\"what s next permalink\">»</a><a class=\"__target-h\" id=\"what-s-next\" aria-hidden></a>What’s Next</h2>\n<p>These updates are very exciting for us, and as we look into the future road map we know that they are even more powerful as we develop dynamic host catalogs and session recording for the Kubernetes ecosystem.</p>\n<p>The latest release wraps up the story for secure software-defined perimeter for Kubernetes, both the API and workloads on top. We built a new deployment example for Kubernetes in our <a href=\"https://github.com/hashicorp/boundary-reference-architecture/tree/main/deployment/kube\">reference architecture repo</a>. You can use this as an example or for getting started with HashiCorp Boundary on Kubernetes.</p>","author":"Jeff Malnick","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"a03bcf4b1f76b650fa923bb5b4d8851453e6195904aa1d888947d9b984de04cb","category":"Tech"}