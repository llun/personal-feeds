{"title":"Retrieve HashiCorp Vault Secrets with Kubernetes CSI","link":"https://www.hashicorp.com/blog/retrieve-hashicorp-vault-secrets-with-kubernetes-csi","date":1620226800000,"content":"<p>With the release of <a href=\"https://www.hashicorp.com/blog/vault-1-7\">HashiCorp Vault 1.7</a>, we have removed the “experimental” note from our <a href=\"https://github.com/hashicorp/vault-csi-provider\">Vault Provider for Secrets Store CSI Driver</a> project and it is now in beta. This project started with <a href=\"https://github.com/hashicorp/vault/issues/7365\">this request</a> to gauge the level of interest in using CSI to expose secrets on a volume within a Kubernetes pod.</p>\n<p>In this article, I’ll give some background on CSI drivers, compare the sidecar and Vault CSI provider methods for Vault secrets retrieval in Kubernetes, and then walk through the process of implementing the Vault CSI provider.</p>\n<h2><a href=\"#kubernetes-and-the-secrets-store-csi-driver\">»</a><a></a>Kubernetes and the Secrets Store CSI Driver</h2>\n<p>To give some background, the <a href=\"https://github.com/container-storage-interface/spec/blob/master/spec.md\">Container Storage Interface, or CSI</a>, is a standard specification for exposing storage systems to containerized workloads. This specification enables storage providers to write standard plugins to integrate their storage systems into container orchestration systems, like Kubernetes. CSI was added to <a href=\"https://kubernetes-csi.github.io/docs/#introduction\">Kubernetes v1.13</a>, which supports CSI spec<a href=\"https://github.com/container-storage-interface/spec/releases/tag/v0.3.0\"> v0.3.0</a> and<a href=\"https://github.com/container-storage-interface/spec/releases/tag/v1.0.0\"> v1.0.0</a>.</p>\n<p>The Kubernetes project maintains a <a href=\"https://kubernetes-csi.github.io/docs/drivers.html\">list of supported CSI drivers</a>. The one that you will be using here is the <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver\">Secrets Store CSI Driver</a>. The Secrets Store CSI driver is different from most other drivers. Instead of integrating directly with the defined backend storage, the Secrets Store CSI driver is pluggable and depends on an additional provider to be defined.</p>\n<p>There are <a href=\"https://secrets-store-csi-driver.sigs.k8s.io/#supported-providers\">supported providers</a> for Microsoft Azure, Google Cloud, and HashiCorp Vault. Each of these supported providers work in conjunction with the Secret Store CSI Driver and are configured with their own parameters.</p>\n<h2><a href=\"#vault-csi-provider-vs-sidecar-injection\">»</a><a></a>Vault CSI Provider vs. Sidecar Injection</h2>\n<p>Using Kubernetes CSI and the Vault CSI provider is an alternative to our <a href=\"https://www.vaultproject.io/docs/platform/k8s/injector\">sidecar injector</a> method, which is one popular way to allow Kubernetes applications to retrieve secrets from Vault with no native Vault logic built-in. Both are excellent methods for retrieving sensitive information from Vault and presenting it to your application pods without the need for additional application changes, and they both help you practice the <a href=\"https://en.wikipedia.org/wiki/Principle_of_least_privilege\">principle of least privilege</a>.</p>\n<p>The sidecar method requires init and/or sidecar containers to retrieve secrets. This is done either by adding pod annotations or using configuration maps defining the Vault role and the path to the secret. This increases the total number of containers running in your cluster but provides a Vault agent to template secrets into configuration files and easily enables rotation of those secrets. An important difference is that the sidecar injector method cannot facilitate syncing of secrets to environment variables.</p>\n<p>The CSI method simplifies this architecture since it does not require any sidecar containers. The Vault provider is deployed as a DaemonSet and renders secrets before the pod starts. It also provides a method to sync secrets into environment variables and Kubernetes secrets. If your security requirements require you to disable <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\"><code>hostPath</code></a> volumes, you should be aware that this method uses <code>hostPath</code> volumes to communicate with the CSI driver. Some Kubernetes distributions may disable this due to the level of access it gives to the node’s filesystem.</p>\n<p>The remainder of this post focuses on the CSI method. I recommend understanding both the injector and CSI methods before determining which will best fit your architecture. To learn more about the sidecar injection method, visit our <a href=\"https://learn.hashicorp.com/tutorials/vault/agent-kubernetes?in=vault/kubernetes\">Learn site</a>, <a href=\"https://www.vaultproject.io/docs/platform/k8s\">documentation</a>, and <a href=\"https://medium.com/hashicorp-engineering/hashicorp-vault-delivering-secrets-with-kubernetes-1b358c03b2a3\">Medium articles</a> by our solutions engineering community.</p>\n<h2><a href=\"#csi-method-updates-and-caveats\">»</a><a></a>CSI Method Updates and Caveats</h2>\n<p>The Vault CSI Provider received a number of improvements around stability, support for all secret engines, and the ability to authenticate with a requesting pod’s service account. The ability to use the pod’s service account provides more granular flexibility with access controls and grants the ability to practice principles of least privilege between applications. You can see the complete list of changes in the <a href=\"https://github.com/hashicorp/vault-csi-provider/pull/80\">Vault CSI provider v0.1.0 release notes</a>.</p>\n<p>Although the project will be supported as a beta, it relies on alpha Kubernetes APIs and the alpha CSI secrets store driver. Where possible, we will provide upgrade paths and deprecation notices for future releases, but remember this project is dependent upon those alpha APIs.</p>\n<h2><a href=\"#the-vault-csi-provider-end-to-end-process\">»</a><a></a>The Vault CSI Provider End-to-End Process</h2>\n<p>The Secrets Store CSI driver communicates with the Vault CSI provider using gRPC to retrieve secret content. This driver enables us to mount multiple secrets, keys, and certs from Vault and present those into our pods as a volume. It uses a <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">custom resource definition (CRD)</a> called <a href=\"https://secrets-store-csi-driver.sigs.k8s.io/getting-started/usage.html\"><code>SecretProviderClass</code></a> to specify Vault as the provider along with the configuration parameters for Vault and your secret paths.</p>\n<p>When the pod is started it will authenticate with the <a href=\"https://www.vaultproject.io/docs/auth/kubernetes\">Kubernetes Auth Method</a> using the service account identity provided in your pod manifest. After a successful authentication, the secret paths defined in your <code>SecretProviderClass</code> will be retrieved and written to a tmpfs volume and mounted to your pod. Now your applications can read those secrets from the mounted volume and use them as necessary.</p><img src=\"https://www.datocms-assets.com/2885/1620161764-csi-secrets-vault-k8s.png\" /><p>This diagram illustrates the end-to-end process of an application pod using the CSI Secrets Store Driver and Vault CSI provider to mount a volume with secrets from Vault.</p>\n<h2><a href=\"#vault-csi-provider-tutorial\">»</a><a></a>Vault CSI Provider Tutorial</h2>\n<p>The goal of this post is to keep things as basic as possible while still allowing you to apply what you have learned to your own architecture. You will go through a development setup in which you run Vault, <a href=\"https://www.vaultproject.io/docs/concepts/dev-server\">in dev mode</a>, in Kubernetes. Running Vault in dev mode is <strong>NOT</strong> intended for production and is <strong>ONLY</strong> for development and experimentation.</p>\n<p>Not every organization runs Vault in Kubernetes, but they still leverage Vault as the central source of truth for sensitive material. You can run Vault <a href=\"https://www.vaultproject.io/docs/install\">on virtual machines</a>, <a href=\"https://www.vaultproject.io/docs/platform/aws-mp\">cloud marketplace images</a>, or the <a href=\"https://cloud.hashicorp.com/\">HashiCorp Cloud Platform</a>. Having the flexibility to run Vault in the architecture that best fits your needs is what makes it such a powerful tool. If you prefer to run an external Vault, you can skip the Vault install and configuration steps and jump ahead to our <a href=\"https://learn.hashicorp.com/tutorials/vault/agent-kubernetes?in=vault/kubernetes\">HashiCorp Learn</a> material for that part of the setup.</p>\n<h3><a href=\"#prerequisites\">»</a><a></a>Prerequisites</h3>\n<p>For the example configuration, you will be using <a href=\"https://minikube.sigs.k8s.io/docs/\">minikube</a>. You will be able to follow the same steps using your favorite flavor of Kubernetes. You can follow the <a href=\"https://minikube.sigs.k8s.io/docs/\">Get Started</a> docs to install minikube or just make sure you have the required prerequisites:</p>\n<pre><code>$ brew <span>install</span> minikube\n</code></pre>\n<p>You will also need to install Kubernetes CLI (kubectl) and Helm:</p>\n<pre><code>$ brew <span>install</span> kubernetes-cli\n$ brew <span>install</span> helm\n</code></pre>\n<p>Now start your minikube cluster:</p>\n<pre><code>$ minikube start\n$ minikube status\n</code></pre>\n<p>This may take a few minutes to start. Just be patient and confirm everything is “Running”.</p>\n<h3><a href=\"#run-vault-in-kubernetes\">»</a><a></a>Run Vault in Kubernetes</h3>\n<p>Getting Vault up and running in Kubernetes is extremely easy when using the provided <a href=\"https://www.vaultproject.io/docs/platform/k8s/helm\">Helm chart</a>. This is our recommended way to install and configure Vault in Kubernetes. We have published tutorials for various <a href=\"https://learn.hashicorp.com/collections/vault/kubernetes\">Vault integrations with Kubernetes</a>, including one for <a href=\"https://learn.hashicorp.com/tutorials/vault/kubernetes-secret-store-driver?in=vault/kubernetes\">CSI</a> that could be helpful to reference.</p>\n<p>You will start by adding the Vault Helm chart repository and running the Helm chart. You can enable the CSI provider via the Helm chart with the added <code>csi.enabled=true</code> parameter.  If you prefer to install the Vault CSI provider via the manifests, a section below has details.</p><pre><code>$ kubectl create ns vault\nnamespace/vault created</code></pre><pre><code>$ helm repo add hashicorp https://helm.releases.hashicorp.com\n$ help repo update # You may need to update\n$ helm install vault hashicorp/vault --namespace=vault \\\n --set \"server.dev.enabled=true\" \\\n --set \"injector.enabled=false\" \\\n --set \"csi.enabled=true\"</code></pre><p>Make sure the Vault container is running and reports “Ready 1/1\":</p><pre><code>$ kubectl get pods --namespace=vault\nNAME               READY  STATUS   RESTARTS  AGE\nvault-0            1/1    Running  0         35m\nvault-csi-provider 1/1    Running  0         35m # If installed via Helm</code></pre><p>Since you are running Vault in dev mode, it will automatically enable the KV-V2 secrets engine at the path <code>/secret</code>. In a production cluster, this would need to be enabled and you could specify an optional path.</p>\n<h3><a href=\"#configure-vault\">»</a><a></a>Configure Vault</h3>\n<p>Now that you have your Vault cluster pod running, you are going to add a test secret that you will retrieve later. The following Vault commands will be run from the container's shell:</p><pre><code>$ # Get a shell in the vault container\n$ kubectl exec -it vault-0 --namespace=vault -- /bin/sh\n$ # Create the KV pair\n$ vault kv put secret/db-pass password=\"db-secret-password\"\nKey              Value\n---              -----\ncreated_time     2021-03-29T21:06:29.46397907Z\ndeletion_time    n/a\ndestroyed        false\nversion          1</code></pre><p>Test that you can retrieve the KV pair you just created:</p><pre><code>$ vault kv get secret/db-pass\n====== Metadata ======\nKey              Value\n---              -----\ncreated_time     2021-03-29T21:06:29.46397907Z\ndeletion_time    n/a\ndestroyed        false\nversion          1\n====== Data ======\nKey         Value\n---         -----\npassword    db-secret-password</code></pre><p>Now you need to enable the authentication method that will be used to verify the identity of the service. This is done by using the <a href=\"https://www.vaultproject.io/docs/auth/kubernetes\">Kubernetes auth method</a>:</p><pre><code>$ vault auth enable kubernetes\n\nSuccess! Enabled kubernetes auth method at: kubernetes/</code></pre><p>Next you need to configure the <a href=\"https://www.vaultproject.io/docs/auth/kubernetes\">Kubernetes auth method</a>. With the recent updates, you will need to make sure to add the <code>issuer</code> parameter. If you miss this step, you will get a <code>claim \"iss\" is invalid</code> error when attempting to start your pod. Since you are using the pod’s service account for authentication, it creates a short-lived bound service account token. Due to this change, the issuer is different from the <code>kubernetes/serviceaccount</code> issuer that default tokens are created with, so validation with Kubenetes’ default issuer will fail. The following parameters will use the Kubernetes service token from the Vault pod for authentication:</p>\n<pre><code>$ vault write auth/kubernetes/config \\\nissuer=\"https://kubernetes.default.svc.cluster.local\" \\\ntoken_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\nkubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\\nkubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n\nSuccess! Data written to: auth/kubernetes/config\n</code></pre>\n<p>Next, you create the Vault policy that you will use to allow your Kubernetes services access to your created secrets paths. This would be the KV pair you created a few steps back:</p>\n<pre><code>$ vault policy write csi - &lt;&lt;EOF\npath \"secret/data/db-pass\" {\n capabilities = [\"read\"]\n}\nEOF\n\nSuccess! Uploaded policy: csi\n</code></pre>\n<p>The last step for the Vault configuration is to create the authentication role. This defines the authentication endpoint path, the Kubernetes service account, the Kubernetes namespace, the access policy you created, and a Vault token TTL (time-to-live):</p>\n<pre><code>$ vault write auth/kubernetes/role/csi \\\n   bound_service_account_names=myapp1-sa \\\n   bound_service_account_namespaces=myapp1 \\\n   policies=csi \\\n   ttl=20m\nSuccess! Data written to: auth/kubernetes/role/database\n</code></pre>\n<p>Now you can exit the Vault container shell:</p>\n<pre><code>$ exit\n</code></pre>\n<h3><a href=\"#install-the-secrets-store-csi-driver-in-kubernetes\">»</a><a></a>Install the Secrets Store CSI Driver in Kubernetes</h3>\n<p>There are two ways to set up the Secrets Store CSI driver in Kubernetes. The <a href=\"https://secrets-store-csi-driver.sigs.k8s.io/getting-started/installation.html\">documentation</a> goes into detail on both the Helm install and using the deploy manifests. This example uses the provided Helm chart and you define the namespace. The default install namespace could vary depending on the install method. Defining the namespace will guarantee all Vault components are in the same namespace. For additional Helm install parameters, refer to the <a href=\"https://github.com/kubernetes-sigs/secrets-store-csi-driver/tree/master/charts/secrets-store-csi-driver\">project documentation</a>:</p><pre><code>$ helm install csi secrets-store-csi-driver/secrets-store-csi-driver --namespace=vault\nNAME: csi\n...</code></pre><p>You can describe the DaemonSet to get more details on the versions of the images used. At the time of writing, I was using driver version v0.0.21.</p><pre><code>$ kubectl describe ds csi-secrets-store --namespace=vault|grep csi-secrets-store/driver\n   Image:      k8s.gcr.io/csi-secrets-store/driver:v0.0.21</code></pre><p>Now that you have the CSI driver setup in your Kubernetes cluster, you have a new custom resource for defining the <code>SecretProviderClass</code>. You can see the new custom resources by listing the custom resource definitions:</p><pre><code>$ kubectl get crd\nNAME                                                        CREATED AT\nsecretproviderclasses.secrets-store.csi.x-k8s.io            2021-03-29T22:29:24Z\nsecretproviderclasspodstatuses.secrets-store.csi.x-k8s.io   2021-03-29T22:29:24Z</code></pre><p>You should also have a DaemonSet and a three-container pod running:</p><pre><code>$ kubectl get ds --namespace=vault\nNAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ncsi-secrets-store-csi-driver   1         1         1       1            1           kubernetes.io/os=linux   57s\nvault-csi-provider             1         1         1       1            1                              4m44s\n\n$ kubectl get pods --namespace=vault\nNAME                                 READY   STATUS    RESTARTS   AGE\ncsi-secrets-store-csi-driver-b7rjj   3/3     Running   0          81s\nvault-0                              1/1     Running   0          5m8s\nvault-csi-provider-bcj4q             1/1     Running   0          5m8s</code></pre><h2><a href=\"#application-namespace-setup\">»</a><a></a>Application Namespace Setup</h2>\n<p>The last thing to do is set up your application namespaces with a ServiceAccount and <code>SecretProviderClass</code>. Each unique Kubernetes namespace will require both. This will allow segmentation of sensitive Vault data between your Kubernetes namespaces via Vault roles and policies. You will define Vault as your provider and point to the Vault pod that you previously set up. You will also define your secret that you created earlier.</p>\n<p>Create the application namespace:</p><pre><code>$ kubectl create ns myapp1\nnamespace/myapp1 created</code></pre><p>Then create the CSI service account for your application namespace:</p>\n<pre><code>$ cat &lt;&lt;EOF | kubectl apply --namespace=myapp1 -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n name: myapp1-sa\n namespace: myapp1\nEOF\n\nserviceaccount/myapp1-sa created\n</code></pre>\n<p>Now you will create the <code>SecretProviderClass</code>. The <code>vault-database</code> metadata name is how you will reference this resource in your pod spec in a later step. You can see your provider is set to Vault, your role name matches the role you created previously, and the Vault address is defined.</p>\n<p>In this example, you are not using Vault namespaces or TLS. I included these additional parameters in this example below but commented them out for reference. Since you are running Vault in dev mode, TLS is not enabled. This is another reason I wouldn’t recommend dev mode for production.</p>\n<p>The array of objects will be the secret paths you want to mount in your pod.</p>\n<ul>\n<li>\n<a></a>\n<a href=\"#objectname\"><code>objectName</code></a>: The secret will be written to a filename in your pod with this alias.</li>\n<li>\n<a></a>\n<a href=\"#secretpath\"><code>secretPath</code></a>: The path in Vault where the secret should be retrieved.</li>\n<li>\n<a></a>\n<a href=\"#secretkey\"><code>secretKey</code></a>: Optional key to read from <code>.Data</code>. The entire JSON payload will be retrieved if this is omitted.</li>\n</ul>\n<pre><code>$ cat &lt;&lt;EOF | kubectl apply --namespace=myapp1 -f -\napiVersion: secrets-store.csi.x-k8s.io/v1alpha1\nkind: SecretProviderClass\nmetadata:\n name: vault-database\nspec:\n provider: vault\n parameters:\n   roleName: \"csi\"\n   vaultAddress: \"http://vault.vault:8200\"\n   # vaultNamespace: &lt;name of Vault Namespace&gt;\n   # vaultCACertPath: &lt;path to CA file for validation&gt;\n   # vaultTLSClientCertPath: &lt;path to client cert&gt;\n   # vaultTLSClientKeyPath: &lt;path to client key&gt;\n   objects: |\n     - objectName: \"password\"\n       secretPath: \"secret/data/db-pass\"\n       secretKey: \"password\"\nEOF\n</code></pre>\n<h3><a href=\"#testing\">»</a><a></a>Testing</h3>\n<p>Finally, you can define a basic NGINX pod and have it use the CSI driver and the <code>SecretProviderClass</code> you created. Then you just need to mount the volume at a defined path:</p>\n<pre><code>$ cat &lt;&lt;EOF | kubectl apply --namespace=myapp1 -f -\nkind: Pod\napiVersion: v1\nmetadata:\n name: nginx-secrets-store-inline\nspec:\n containers:\n - image: nginx\n   name: nginx\n   volumeMounts:\n   - name: secrets-store-inline\n     mountPath: \"/mnt/secrets-store\"\n     readOnly: true\n serviceAccountName: myapp1-sa\n volumes:\n   - name: secrets-store-inline\n     csi:\n       driver: secrets-store.csi.k8s.io\n       readOnly: true\n       volumeAttributes:\n         secretProviderClass: \"vault-database\"\nEOF\n</code></pre>\n<p>You will want to make sure your NGINX pod is running. If it is not, this is a quick sign that something didn’t work. If you don’t already have the NGINX image downloaded, it will take a few minutes longer.</p><pre><code>$ kubectl get pods -n myapp1\nNAME                         READY   STATUS    RESTARTS   AGE\nnginx-secrets-store-inline   1/1     Running   0          22m</code></pre><p>If your pod is stuck with a <code>ContainerCreating</code> status, check the events of the pod to begin troubleshooting what went wrong. If you copied and pasted the commands in this post, you should be good to go.</p>\n<p>A successful startup would look like this:</p><pre><code>$ kubectl describe pod -n myapp1 nginx-secrets-store-inline\n...\nEvents:\nType    Reason     Age   From               Message\n----    ------     ----  ----               -------\nNormal  Scheduled  34m   default-scheduler  Successfully assigned myapp1/nginx-secrets-store-inline to minikube\nNormal  Pulling    33m   kubelet            Pulling image \"nginx\"\nNormal  Pulled     33m   kubelet            Successfully pulled image \"nginx\" in 6.469146131s\nNormal  Created    33m   kubelet            Created container nginx\nNormal  Started    33m   kubelet            Started container nginx</code></pre><p>Now that the test pod is up and running, you can read the contents of your mounted volume:</p><pre><code>$ kubectl exec -n=myapp1 nginx-secrets-store-inline -- cat /mnt/secrets-store/password\ndb-secret-password</code></pre><h2><a href=\"#conclusion-and-looking-ahead\">»</a><a></a>Conclusion and Looking Ahead</h2>\n<p>We are excited to offer the CSI method alongside our sidecar injection option to help secure your Kubernetes application secrets with Vault. In this post, you learned how both options use different approaches to accomplish this goal.</p>\n<p>The sidecar injection method requires additional sidecar containers and is set up via pod annotations. With this method the secret stays within the context of the pod. The CSI method does not require sidecar containers and is set up via a CRD. This method uses Kubernetes <code>hostPath</code> volumes, which puts the secret into the context of the node. Understanding these differences and how they impact your architecture can help you choose the best approach to fit both your security and automation requirements.</p>\n<p>Going forward, the focus for the Vault CSI Provider is moving from beta to GA status and watching as the related Kubernetes APIs and CSI secrets store driver move from alpha, to beta, and finally to GA. The developers are also looking to <a href=\"https://github.com/hashicorp/vault-csi-provider/issues/82\">move Vault connection details into the provider pod's configuration</a>. You can download, test, and keep up with the <a href=\"https://github.com/hashicorp/vault-csi-provider\">Vault CSI Provider project on GitHub</a>.</p>","author":"Troy Fluegge","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"8f650327559ef38da2a85ef86bf5e64ca47e06c2a7f645932e1af004fbb21717","category":"Tech"}