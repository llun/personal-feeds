{"title":"Autopilot: Simplifying the Integrated Storage Experience with HashiCorp Vault","link":"https://www.hashicorp.com/blog/autopilot-simplifying-integrated-storage-with-hashicorp-vault","date":1618243200000,"content":"<p>We introduced official support for <a href=\"https://www.vaultproject.io/docs/concepts/integrated-storage\">Integrated Storage</a> in HashiCorp Vault 1.4, which allows Vault admins to configure an internal storage option for storing Vault’s persistent data rather than using an external storage backend (via the <a href=\"https://raft.github.io/raft.pdf\">Raft</a> consensus protocol). With each subsequent Vault release, we have continued to improve the operational experience and we are pleased to announce a highly requested feature called Autopilot in Vault 1.7.</p>\n<p>Integrated Storage eliminates much of the operational overhead of managing a separate storage backend and avoids the additional networking imposed by these separate systems. This, in turn, reduces the complexity of the Vault cluster deployment and also makes the diagnosis and troubleshooting of issues easier, reducing mean-time-to-detect for issues and mean-time-to-restore for customers.</p>\n<h2><a href=\"#the-integrated-storage-journey\">»</a><a></a>The Integrated Storage Journey</h2>\n<p>The Vault team has added several enhancements over the last few releases to improve deployment of Integrated Storage clusters.</p>\n<ul>\n<li><strong><a href=\"https://learn.hashicorp.com/tutorials/vault/raft-ha-storage?in=vault/raft\">High Availability (HA) coordination</a></strong>: Introduced in Vault 1.5. Provides the ability to use Integrated Storage for HA when you need to use a storage backend that does not support HA, such as Amazon S3, Cassandra, and MSSQL.</li>\n<li><strong><a href=\"https://www.vaultproject.io/docs/configuration/storage/raft\">Cloud Auto-join</a></strong>: Introduced in Vault 1.6. Enables auto-discovery of Integrated Storage peers when working in a cloud environment. Auto-join allows new Vault nodes to automatically join a Vault cluster.</li>\n<li><strong><a href=\"https://www.vaultproject.io/docs/enterprise/automated-integrated-storage-snapshots\">Automated Snapshots</a></strong>: Introduced in Vault 1.6. Lets customers take automated and scheduled data snapshots of Vault Integrated Storage clusters at different points of time.</li>\n</ul>\n<p>These features made Integrated Storage easier to use with cloud environments and snapshots. However, operators still had to use manual methods to monitor the health of a cluster, ensure cluster stability when nodes are added (or removed), and clean up failed nodes in a cluster.</p>\n<h2><a href=\"#improving-the-operator-experience-with-autopilot\">»</a><a></a>Improving the Operator Experience with Autopilot</h2>\n<p>Vault 1.7 was made <a href=\"https://www.hashicorp.com/blog/vault-1-7\">publicly available</a> on March 25, 2021. This release introduced support for the Autopilot features in Vault open source. Autopilot, as the name itself suggests, will help automate and simplify Vault operator and admin workflows for monitoring and operating Vault Integrated Storage clusters.</p>\n<pre><code>$ vault operator raft autopilot get-config\n\nKey                                   Value\n---                                   -----\nCleanup Dead Severs                   false\nLast Contact Threshold                10s\nDead Server Last Contact Threshold    24h0m0s\nServer Stabilization Time             10s\nMin Quorum                            0\nMax Trailing Logs                     1000\n</code></pre>\n<p>Autopilot is enabled by default with Integrated Storage clusters using Vault 1.7. Note, though, dead server cleanup is not enabled by default, it must be explicitly enabled. The primary pain points that Autopilot helps alleviate for operators are elaborated below.</p>\n<ul>\n<li>Autopilot provides improved insight into cluster and node state via a new <a href=\"https://www.vaultproject.io/docs/commands/operator/raft#autopilot-state\">State API</a> that allows operators to know the node and overall cluster health, the list of nodes in a cluster (by node ID and IP address), the node type (leader, voter, non-voter) and the failure tolerance of the cluster. The health of a node is determined by:\n<ul>\n<li>“Last Contact Threshold,” which specifies the maximum amount of time a server can go without contact with the leader node before being deemed unhealthy.</li>\n<li>“Max Trailing Logs,” which specifies the maximum number of log entries in the Raft log that a server can trail the leader by before being considered unhealthy.</li>\n</ul>\n</li>\n</ul>\n<pre><code>$ vault operator raft autopilot state\n\nHealthy:                  true\nFailure Tolerance:        1\nLeader:                   raft1\nVoters:\n  raft1\n  raft2\n  raft3\nServers:\n  raft1\n    Name:             raft1\n    Address:          127.0.0.1:8201\n    Status:           leader\n    Node Status:      alive\n    Healthy:          true\n    Last Contact:     0s\n    Last Term:        3\n    Last Index:       38\n</code></pre>\n<ul>\n<li>\n<p>Autopilot ensures cluster stability when new nodes join a cluster. A newly joined voter node is initially added to a cluster as a “non-voter,” and its state is monitored for the configured “server stabilization time” period. If the node stays healthy for that period, the node is promoted to “voter” status. This ensures that an unstable new node does not disrupt the entire cluster, and is handled without requiring operator intervention.</p>\n</li>\n<li>\n<p>Autopilot takes away from Vault operators the burden of monitoring and cleaning up failed servers. Dead server cleanup, which needs to be explicitly enabled via the API, periodically scans the cluster and automatically cleans up failed servers. The “Dead Server Last Contact Threshold” configuration can be used to tune the time to wait until declaring that the lost node is “failed” and cleaning it up from the configuration. When dead server cleanup is enabled, a min-quorum configuration needs to be provided to configure the minimum number of servers to be retained in a cluster despite enabling dead server cleanup. This is essential so that cluster stability is not impacted due to quorum disruption.</p>\n</li>\n</ul>\n<h2><a href=\"#summary-and-next-steps\">»</a><a></a>Summary and Next Steps</h2>\n<p>With the various features now supported for managing Integrated Storage, operators have access to simple, automated workflows to manage and operate their Vault Integrated Storage clusters. <a href=\"https://cloud.hashicorp.com/docs/vault\">HCP Vault</a>, which is HashiCorp’s managed cloud service for Vault, also uses Integrated Storage for these reasons. Users of Integrated Storage will be able to benefit from the wide variety of workflows that have been tested for the customer-managed and HashiCorp-managed Vault products.</p>\n<p>To get started exploring and using Integrated Storage, please refer to the <a href=\"https://learn.hashicorp.com/collections/vault/raft\">HashiCorp Learn guides</a> and the reference architecture documents as well as the <a href=\"https://www.vaultproject.io/docs/concepts/integrated-storage\">documentation</a>. For more information on Vault, please visit the <a href=\"https://www.vaultproject.io/\">Vault project website</a>. As always, we are very interested in hearing about your experiences with the product, so please share your <a href=\"https://github.com/hashicorp/vault/issues\">feedback</a> so that we can continue to improve our products.</p>","author":"Aarti Iyengar","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"bfb9af7d760e17432b3b37ac6e62a3f409beb53f69c73b5b9b12352e9c60c6e9","category":"Tech"}