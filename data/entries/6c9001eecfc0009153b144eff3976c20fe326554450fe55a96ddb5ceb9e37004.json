{"title":"GKE Cluster Setup with CDK for Terraform","link":"https://www.hashicorp.com/blog/gke-cluster-setup-with-cdk-for-terraform","date":1624478400000,"content":"<p>Building your application layer on top of Kubernetes can be tough. You need to:</p>\n<ul>\n<li>Provision a cluster.</li>\n<li>Get platform-wide services like CertManager, Istio, Grafana, Prometheus, etc. set up.</li>\n<li>Educate developers on how to get their service deployed.</li>\n<li>Ensure everyone is following Kubernetes best practices.</li>\n<li>Glue the tool to deploy the server to the workload deployment tools (Helm or plain kubectl).</li>\n</ul>\n<p>Let’s try to simplify this by using the <a href=\"https://github.com/hashicorp/terraform-cdk\">CDK for Terraform</a>, which allows you to express infrastructure needs in a common programming language. In this tutorial, I’ll show you how to provision and manage a Google Kubernetes Engine (GKE) cluster using TypeScript via the CDK for Terraform.</p>\n<p>I chose Typescript as it’s the language I’m most familiar with, but CDK for Terraform also supports Go, Python, Java, and C#. If you want to learn more about the CDK, please read <a href=\"https://learn.hashicorp.com/tutorials/terraform/cdktf\">our guide.</a> If you want to skip ahead to the full code, you can read through the <a href=\"https://github.com/hashicorp/kubernetes-on-gcp-with-terraform-cdk\">GitHub repo for this tutorial</a> and follow along.</p>\n<h2><a href=\"#setting-up-the-cluster\">»</a><a></a>Setting Up the Cluster</h2>\n<p>First of all, we need to have a GKE cluster set up. We will do this in a separate <a href=\"https://github.com/hashicorp/terraform-cdk/blob/main/docs/working-with-cdk-for-terraform/app-stacks-concept.md\">TerraformStack</a> so that the Terraform state is separated from our application state. We use a class extending the Resource class here that will be our abstraction over a Kubernetes Cluster on GKE. This allows us to enforce methods of operation. Some examples could be setting the service account on each of the node pools or suggesting a default node pool or instance size.</p><pre><code>import {\n  ContainerCluster,\n  ContainerNodePool,\n  ContainerRegistry,\n  DataGoogleContainerCluster,\n  GoogleProvider,\n  ProjectIamMember,\n  ServiceAccount,\n} from \"@cdktf/provider-google\";\nimport { CLUSTER_NAME } from \"./config\";\n\n// https://developers.google.com/identity/protocols/oauth2/scopes\nconst oauthScopes = [\n  \"https://www.googleapis.com/auth/devstorage.read_only\",\n  \"https://www.googleapis.com/auth/logging.write\",\n  \"https://www.googleapis.com/auth/monitoring\",\n  \"https://www.googleapis.com/auth/servicecontrol\",\n  \"https://www.googleapis.com/auth/service.management.readonly\",\n  \"https://www.googleapis.com/auth/trace.append\",\n  \"https://www.googleapis.com/auth/cloud-platform\",\n];\n\nclass KubernetesCluster extends Resource {\n  private sa: ServiceAccount;\n  private cluster: ContainerCluster;\n\n  constructor(scope: Construct, name: string, serviceAccount: ServiceAccount) {\n    super(scope, name);\n\n    this.sa = serviceAccount;\n    this.cluster = new ContainerCluster(this, \"cluster\", {\n      name,\n      removeDefaultNodePool: true,\n      initialNodeCount: 1,\n      nodeConfig: [\n        {\n          preemptible: true,\n          serviceAccount: this.sa.email,\n          oauthScopes,\n        },\n      ],\n    });\n  }\n\n  addNodePool(name: string, nodeCount = 3, machineType = \"e2-medium\") {\n    new ContainerNodePool(this, name, {\n      name,\n      cluster: this.cluster.name,\n      nodeCount,\n      nodeConfig: [\n        {\n          preemptible: true,\n          machineType,\n          serviceAccount: this.sa.email,\n          oauthScopes,\n        },\n      ],\n    });\n  }\n\n  addAutoscalingNodePool(\n    name: string,\n    minNodeCount = 3,\n    maxNodeCount = 10,\n    machineType = \"e2-medium\"\n  ) {\n    new ContainerNodePool(this, name, {\n      name,\n      cluster: this.cluster.name,\n      autoscaling: [\n        {\n          minNodeCount,\n          maxNodeCount,\n        },\n      ],\n      nodeConfig: [\n        {\n          preemptible: true,\n          machineType,\n          serviceAccount: this.sa.email,\n          oauthScopes,\n        },\n      ],\n    });\n  }\n\n\nclass InfrastructureLayer extends TerraformStack {\n  constructor(scope: Construct, name: string) {\n    super(scope, name);\n\n    new GoogleProvider(this, \"google\", {\n      zone: \"us-central1-c\",\n      project: \"dschmidt-cdk-test\",\n    });\n\n    const sa = new ServiceAccount(this, \"sa\", {\n      accountId: \"cluster-admin\",\n      displayName: \"Cluster Admin\",\n    });\n\n    const pushSa = new ServiceAccount(this, \"registry-push\", {\n      accountId: \"registry-push\",\n      displayName: \"RegistryPush\",\n    });\n\n    new ProjectIamMember(this, \"sa-role-binding\", {\n      role: \"roles/storage.admin\",\n      member: `serviceAccount:${sa.email}`,\n    });\n\n    new ProjectIamMember(this, \"push-role-binding\", {\n      role: \"roles/storage.admin\",\n      member: `serviceAccount:${pushSa.email}`,\n    });\n\n    new ContainerRegistry(this, \"registry\", {});\n\n    const cluster = new KubernetesCluster(this, CLUSTER_NAME, sa);\n    cluster.addNodePool(\"main\");\n    cluster.addAutoscalingNodePool(\"workloads\");\n  }\n}\n\nconst app = new App();\nnew InfrastructureLayer(app, \"infrastructure\");\napp.synth();</code></pre><p><span>After setting up the cluster, we’ll create a separate service account that we are going to use to push Docker images to the Google Container Registry. Configure this cluster with two node pools: The first one is fixed in size for the baseline infrastructure, the second one has autoscaling enabled. The cluster we create looks like this:</span></p><img src=\"https://www.datocms-assets.com/2885/1624457718-gkc-cluster.png\" /><p><span>Yes, you can do this in Terraform directly without the CDK for Terraform, but the advantage of doing it in TypeScript is that it helps teammates that are not familiar with HCL to better understand what’s going on and shields some of the underlying complexity from them. Also, if they use a modern editor they get tooltips that link to the Terraform documentation like this one:</span></p><img src=\"https://www.datocms-assets.com/2885/1624457869-text-editor.png\" /><h2><a href=\"#accessing-the-cluster\">»</a><a></a>Accessing the Cluster</h2>\n<p>With the cluster ready, we want to access it to deploy our workloads. To make this super easy, we’ll add a static method to our Kubernetes cluster abstraction to get an abstraction over the cluster. Use the pre-build provider for Google Cloud, which you can download directly through the package management tool of your language.</p>\n<p>Now we need to tap into the power of Terraform and get providers and modules. In this case, we are going to use the Kubernetes and Helm providers as well as the GKEAuth module to authenticate against the GKE Cluster.</p>\n<p>To use these providers and modules we need to configure them in our <code>cdktf.json</code> file:</p><pre><code>{\n  \"language\": \"typescript\",\n  \"app\": \"npm run --silent compile &amp;&amp; node main.js\",\n  \"terraformProviders\": [\n    \"hashicorp/helm@ ~&gt; 2.1.2\"\n    \"hashicorp/kubernetes@ ~&gt; 2.2.0\",\n    \"hashicorp/local@ ~&gt; 2.1.0\",\n\n    // we will come to these a little later\n    \"kreuzwerker/docker@ ~&gt; 2.11.0\",\n    \"hashicorp/null@ ~&gt; 3.1.0\", \n  ],\n  \"terraformModules\": [\"terraform-google-modules/kubernetes-engine/google//modules/auth@ ~&gt; 14.3.0\"],\n  \"context\": {\n    \"excludeStackIdFromLogicalIds\": \"true\",\n    \"allowSepCharsInLogicalIds\": \"true\"\n  }\n}</code></pre><p><span>With this in place, we can run <code>cdktf get</code>. That generates the bindings we can access under <code>./.gen</code>. The folder might change depending on the language you are using.</span></p>\n<p><span>The `onCluster` method uses a <code>DataSource</code> to get information around the GKE cluster. We pass the information down to the GKEAuth module, which configures the Kubernetes and Helm providers.</span></p><pre><code>class KubernetesCluster extends Resource {\n  // see above\n  static onCluster(scope: Construct, name: string) {\n    const cluster = new DataGoogleContainerCluster(scope, \"cluster\", {\n      name,\n    });\n\n    const auth = new GKEAuth(scope, \"auth\", {\n      clusterName: cluster.name,\n      location: cluster.location,\n      projectId: cluster.project,\n    });\n\n    new KubernetesProvider(scope, \"kubernetes\", {\n      clusterCaCertificate: auth.clusterCaCertificateOutput,\n      host: auth.hostOutput,\n      token: auth.tokenOutput,\n    });\n\n    new HelmProvider(scope, \"helm\", {\n      kubernetes: [\n        {\n          clusterCaCertificate: auth.clusterCaCertificateOutput,\n          host: auth.hostOutput,\n          token: auth.tokenOutput,\n        },\n      ],\n    });\n\n    return {\n      installHelmChart(config: ReleaseConfig) {\n        new Release(scope, config.name, config);\n      },\n\n      exposeDeployment(\n        namespace: Namespace,\n        name: string,\n        image: string,\n        labels: Record,\n        dependencies: ITerraformDependable[]\n      ) {\n        return new KubernetesService(\n          scope,\n          namespace,\n          name,\n          image,\n          labels,\n          dependencies\n        );\n      },\n    };\n  }\n}</code></pre><p>We return an object with methods we want to expose for a Kubernetes cluster: one for creating a Helm release, and one for creating an exposed deployment. We will go into detail about both in the next sections.</p>\n<h2><a href=\"#baseline-setup\">»</a><a></a>Baseline Setup</h2>\n<p>To make the cluster production-ready, we need some software that provides fundamental services for our applications to use. Secret management, service mesh capabilities, and user authentication are common services that could be handled once for the entire cluster. These services are commonly deployed using Helm charts. I will deploy CertManager as an example of how to deploy Helm charts. In production you would likely have more chart-based functionality in place.</p><pre><code>class BaselineLayer extends TerraformStack {\n  constructor(scope: Construct, name: string) {\n    super(scope, name);\n    new GoogleProvider(this, \"google\", {\n      zone: \"us-central1-c\",\n      project: \"dschmidt-cdk-test\",\n    });\n\n    const cluster = KubernetesCluster.onCluster(this, CLUSTER_NAME);\n    cluster.installHelmChart({\n      name: \"cert-manager\",\n      repository: \"https://charts.jetstack.io\",\n      chart: \"cert-manager\",\n      createNamespace: true,\n      namespace: \"cert-manager\",\n      version: \"v1.3.1\",\n    });\n  }\n}</code></pre><h2><a href=\"#the-application\">»</a><a></a>The Application</h2>\n<p>So far, we’ve made use of a lot of Terraform functionality, but we haven't tapped into all the power of the CDK for Terraform yet. In our project setup, we have a few services in the <code>../services</code> folder that need to be deployed. We don’t want to force anyone to learn much about Kubernetes or Docker to get their application deployed. We also want to ensure users are following best practices, so let’s see how this can be done:</p><pre><code>import * as fs from \"fs\";\nimport * as path from \"path\";\nimport { DockerProvider } from \"./.gen/providers/docker/docker-provider\";\nimport { Namespace } from \"./.gen/providers/kubernetes/namespace\";\nimport { buildAndPushImage } from \"./docker\";\n\nclass ApplicationLayer extends TerraformStack {\n  constructor(scope: Construct, name: string) {\n    super(scope, name);\n    new GoogleProvider(this, \"google\", {\n      zone: \"us-central1-c\",\n      project: \"dschmidt-cdk-test\",\n    });\n    new DockerProvider(this, \"docker\", {});\n    const cluster = KubernetesCluster.onCluster(this, CLUSTER_NAME);\n\n    const ns = new Namespace(this, \"ns\", {\n      metadata: [\n        {\n          name,\n        },\n      ],\n    });\n\n    const servicePath = path.resolve(__dirname, \"../services\");\n    fs.readdirSync(servicePath).forEach((p) =&gt; {\n      const [tag, image] = buildAndPushImage(\n        this,\n        p,\n        path.resolve(servicePath, p)\n      );\n      cluster.exposeDeployment(\n        ns,\n        p,\n        tag,\n        {\n          application: p,\n        },\n        [image]\n      );\n    });\n  }\n}\n\n\nconst app = new App();\nnew InfrastructureLayer(app, \"infrastructure\");\nnew BaselineLayer(app, \"baseline\");\nnew ApplicationLayer(app, \"development\");\nnew ApplicationLayer(app, \"staging\");\nnew ApplicationLayer(app, \"production\");\napp.synth();</code></pre><p><span>We create one of these application layers for each environment we want to deploy. Each of these layers deploys their namespace and iterates through the service folders. For each of these applications we build and push a Docker image that we then deploy onto the cluster. Let's first see how the image is built:</span></p><pre><code>import { TerraformAsset } from \"cdktf\";\nimport { Construct } from \"constructs\";\nimport * as fs from \"fs\";\nimport * as path from \"path\";\nimport { VERSION, DOCKER_ORG } from \"./config\";\nimport { Resource } from \"./.gen/providers/null/resource\";\nimport {\n  DataGoogleServiceAccount,\n  ServiceAccountKey,\n} from \"@cdktf/provider-google\";\n\nexport function buildAndPushImage(\n  scope: Construct,\n  imageName: string,\n  p: string\n): [string, Resource] {\n  const _ = (name: string) =&gt; `${imageName}-${name}`;\n  const files = fs.readdirSync(p);\n\n  function getDockerfileFlag() {\n    if (files.includes(\"Dockerfile\")) {\n      return \"\";\n    }\n\n    if (files.includes(\"package.json\")) {\n      const asset = new TerraformAsset(scope, _(\"node-dockerfile\"), {\n        path: path.resolve(__dirname, \"Dockerfile.node\"),\n      });\n\n      return `-f ${asset.path}`;\n    }\n\n    if (files.includes(\"Cargo.toml\")) {\n      const asset = new TerraformAsset(scope, _(\"node-dockerfile\"), {\n        path: path.resolve(__dirname, \"Dockerfile.rust\"),\n      });\n\n      return `-f ${asset.path}`;\n    }\n\n    throw new Error(\n      \"Unknown application language, please add a Dockerfile or use node or rust\"\n    );\n  }\n\n  function getVersion(): string {\n    if (files.includes(\"package.json\")) {\n      return require(path.resolve(p, \"package.json\")).version;\n    }\n\n    return VERSION;\n  }\n\n  const dockerfileFlag = getDockerfileFlag();\n  const content = new TerraformAsset(scope, _(\"content\"), {\n    path: p,\n  });\n\n  const sa = new DataGoogleServiceAccount(scope, _(\"sa\"), {\n    accountId: \"registry-push\",\n  });\n\n  const key = new ServiceAccountKey(scope, _(\"sa-key\"), {\n    serviceAccountId: sa.email,\n  });\n\n  const version = getVersion();\n\n  const tag = `gcr.io/${DOCKER_ORG}/${imageName}:${version}-${content.assetHash}`;\n  const image = new Resource(scope, _(\"image\"), {\n    triggers: {\n      tag,\n    },\n  });\n\n  const cmd = `echo '${key.privateKey}' | base64 -D | docker login -u _json_key --password-stdin https://gcr.io &amp;&amp; docker build ${dockerfileFlag} -t ${tag} ${content.path} &amp;&amp; docker push ${tag}`;\n  image.addOverride(\"provisioner.local-exec.command\", cmd);\n\n  return [tag, image];\n}</code></pre><p>We want to support Node.js and Rust projects out of the box, so we put standard Dockerfiles in place. In <code>getDockerfileFlag</code> you can see that we compose the <code>docker build -f</code> command to either use the default Dockerfile inside the context folder or the standard Dockerfile. </p>\n<p>We get the version of the project in the <code>getVersion</code> function so that we can compose the tag for the Docker image. As you can see in the code above, we can act based on local files so we can shape the interface our devs need to interact with however we like. </p>\n<p>Want to provision a PostgreSQL or Redis instance by setting a value in the <code>package.json</code>? You can do it. Want to write a module that defines which environment variables to pass from where, and at the same time delivers these in the application context? No problem. Your devs want more flexibility and want to write their CDK code local to their application? As long as you stay in the same language there is no problem.</p>\n<p>Remember the service account we set up to push images? We are going to use that now to generate <code>ServiceAccountKeys</code>. Armed with the key, we use a Null Resource and the local-exec it provides to run a <code>docker login</code>, <code>build</code>, <code>push</code> chain that gets our image pushed into GCR.</p>\n<p>Now let's talk about deploying the container. In this example we make some assumptions about the service — that it runs on port 80 and has a <code>/health</code> endpoint for health checks. We also assume that every deployment should have its own service. We have the <code>KubernetesService</code> class that deploys the image based on these assumptions. We pass down the image tag and the image resource, so that the deployment resource knows when the image was pushed.</p><pre><code>class KubernetesService extends Resource {\n  constructor(\n    scope: Construct,\n    namespace: Namespace,\n    name: string,\n    image: string,\n    labels: Record,\n    dependencies: ITerraformDependable[]\n  ) {\n    super(scope, name);\n    const deployment = new Deployment(scope, `${image}-deployment`, {\n      dependsOn: dependencies,\n      metadata: [\n        {\n          name,\n          labels,\n          namespace: namespace.id,\n        },\n      ],\n      spec: [\n        {\n          selector: [\n            {\n              matchLabels: labels,\n            },\n          ],\n          template: [\n            {\n              metadata: [\n                {\n                  labels,\n                },\n              ],\n              spec: [\n                {\n                  container: [\n                    {\n                      name: \"application\",\n                      image: image,\n                      port: [{ containerPort: 80 }],\n                      livenessProbe: [\n                        {\n                          httpGet: [\n                            {\n                              path: \"/health\",\n                              port: \"80\",\n                            },\n                          ],\n                        },\n                      ],\n                    },\n                  ],\n                },\n              ],\n            },\n          ],\n        },\n      ],\n    });\n\n    new Service(scope, `${image}-service`, {\n      dependsOn: [deployment],\n      metadata: [{ name: image, namespace: namespace.id }],\n      spec: [\n        {\n          selector: { application: image },\n          port: [{ port: 80 }],\n        },\n      ],\n    });\n  }\n}</code></pre><p><span>Running <code>cdktf apply &lt;stack&gt;</code> for development, staging, and production deploys the workloads directly on the cluster.</span></p><img src=\"https://www.datocms-assets.com/2885/1624459425-workloads.png\" /><h2><a href=\"#summary\">»</a><a></a>Summary</h2>\n<p>In this tutorial, we set out to...</p>\n<ul>\n<li>Create a GKE cluster</li>\n<li>Set up platform-wide services like CertManager, Istio, Grafana, Prometheus, etc.</li>\n<li>Educate developers on how to get their service deployed</li>\n<li>Ensure everyone is following Kubernetes best practices</li>\n<li>Glue the tool to deploy the server to the workload deployment tools (Helm or plain kubectl)</li>\n</ul>\n<p>To some degree, this walkthrough achieves all of these. And now we can now see why the CDK for Terraform is so useful.</p>\n<p>It allows you to find abstractions over infrastructure that fit your team. Which abstractions you choose depend on the company, the setup, and the familiarity of your developers with infrastructure, but you have lots of choices. I am certain the abstractions I chose are not the right ones for your use case. Luckily it’s quite easy to change them and introduce new ones that work for you.</p>\n<p>The last thing to mention is that, in general, infrastructure automation should be run inside of a CI engine (e.g. Google Cloud Build if you are working in GCP already), so that the access and execution environment can be controlled in one place.</p>\n<h2><a href=\"#what-s-next\">»</a><a></a>What's Next?</h2>\n<p>If you want to learn more about the CDK for Terraform you can visit these resources:</p>\n<ul>\n<li><a href=\"https://learn.hashicorp.com/tutorials/terraform/cdktf\">The CDK for Terraform guide series on HashiCorp Learn</a></li>\n<li><a href=\"https://learn.hashicorp.com/tutorials/terraform/cdktf-build?in=terraform/cdktf\">How to build AWS infrastructure in TypeScript</a></li>\n<li><a href=\"https://learn.hashicorp.com/tutorials/terraform/cdktf-build-python?in=terraform/cdktf\">How to build AWS infrastructure in Python</a></li>\n<li><a href=\"https://learn.hashicorp.com/tutorials/terraform/cdktf-build-go?in=terraform/cdktf\">How to build AWS infrastructure in Go</a></li>\n<li><a href=\"https://github.com/hashicorp/terraform-cdk#examples\">Various examples of CDK for Terraform in action</a></li>\n</ul>\n<p>For feedback you can send me a <a href=\"https://twitter.com/DSchmidt1992\">DM on Twitter</a> or an email to dschmidt(at)hashicorp.com.</p>","author":"Daniel Schmidt","siteTitle":"HashiCorp Blog","siteHash":"219aa6310b3388f2335eba49871f4df9581f2c58eaeb5e498363b54e835b7001","entryHash":"6c9001eecfc0009153b144eff3976c20fe326554450fe55a96ddb5ceb9e37004","category":"Tech"}