{"title":"The efforts to make text-based AI less racist and terrible","link":"https://arstechnica.com/?p=1774477","date":1624104334000,"content":"<div>\n<figure><img src=\"https://cdn.arstechnica.net/wp-content/uploads/2021/06/open-ai-800x533.jpg\" /><p><a href=\"https://cdn.arstechnica.net/wp-content/uploads/2021/06/open-ai.jpg\">Enlarge</a> (credit: Getty Images)</p>  </figure><div><a name=\"page-1\"></a></div>\n<p>In July 2020, OpenAI launched GPT-3, an<a href=\"https://www.wired.com/tag/artificial-intelligence/\"> artificial intelligence</a> language model that quickly stoked excitement about computers writing poetry, news articles, and programming code. Just as quickly, it was shown to sometimes be foulmouthed and toxic. OpenAI said it was working on fixes, but the company recently discovered GPT-3 was being used to<a href=\"https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/\"> generate child porn</a>.</p>\n<p>Now <a href=\"https://www.wired.com/tag/openai/\">OpenAI</a> researchers say they’ve found a way to curtail GPT-3’s toxic text by feeding the program roughly 100 encyclopedia-like samples of writing by human professionals on topics like history and technology but also abuse, violence, and injustice.</p>\n<p>OpenAI’s project shows how the tech industry is scrambling to constrain the dark side of a technology that’s shown enormous potential but also can spread disinformation and perpetuate biases. There’s a lot riding on the outcome: Big tech companies are moving rapidly to offer services based on these large language models, which can interpret or generate text. Google calls them <a href=\"https://www.wired.com/story/google-hopes-ai-turn-search-conversation/\">central to the future of search</a>, and Microsoft is using <a href=\"https://www.wired.com/story/ai-write-code-ordinary-language/\">GPT-3 for programming</a>. In a potentially more ominous development, groups are working on<a href=\"https://www.wired.com/tag/open-source/\"> open source</a> versions of these language models that could exhibit the same weaknesses and share them more widely. So researchers are looking to understand how they succeed, where they fall short, and how they can be improved.</p></div><p><a href=\"https://arstechnica.com/?p=1774477#p3\">Read 21 remaining paragraphs</a> | <a href=\"https://arstechnica.com/?p=1774477&amp;comments=1\">Comments</a></p>","author":"WIRED","siteTitle":"Ars Technica","siteHash":"5b0ddf6e8923e49262a7894cfd77962733e43fbcc565a103b48373820b310636","entryHash":"62647d5980e62d3b69ea51fd2e10bb8e1bc850484a3b25f4cfbb9d48ed431614","category":"Tech"}