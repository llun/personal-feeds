{"title":"Flaky Tests: Getting Rid Of A Living Nightmare In Testing","link":"https://smashingmagazine.com/2021/04/flaky-tests-living-nightmare/","date":1617793200000,"content":"<p>There is a fable that I think about a lot these days. The fable was told to me as a child. It’s called “The Boy Who Cried Wolf” by Aesop. It is about a boy who tends the sheep of his village. He gets bored and pretends that a wolf is attacking the flock, calling out to the villagers for help — only for them to disappointedly realize that it is a false alarm and leave the boy alone. Then, when a wolf actually appears and the boy calls for help, the villagers believe it is another false alarm and do not come to the rescue, and the sheep end up getting eaten by the wolf.</p>\n<p>The moral of the story is best summarized by the author himself:</p>\n<blockquote>“A liar will not be believed, even when he speaks the truth.”</blockquote>\n\n<p>A wolf attacks the sheep, and the boy cries for help, but after numerous lies, no one believes him anymore. This moral can be applied to testing: Aesop’s story is a nice allegory for a matching pattern that I stumbled upon: flaky tests that fail to provide any value.</p>\n<h3>Front-End Testing: Why Even Bother?</h3>\n<p>Most of my days are spent on front-end testing. So it shouldn’t surprise you that the code examples in this article will be mostly from the front-end tests that I’ve come across in my work. However, in most cases, they can be easily translated to other languages and applied to other frameworks. So, I hope the article will be useful to you — whatever expertise you might have.</p>\n<p>It’s worth recalling what front-end testing means. In its essence, front-end testing is a set of practices for testing the UI of a web application, including its functionality.</p>\n<p>Starting out as a quality-assurance engineer, I know the pain of endless manual testing from a checklist right before a release. So, in addition to the goal of ensuring that an application remains error-free during successive updates, I strived to <strong>relieve the workload of tests</strong> caused by those routine tasks that you don’t actually need a human for. Now, as a developer, I find the topic still relevant, especially as I try to directly help users and coworkers alike. And there is one issue with testing in particular that has given us nightmares.</p>\n<h3>The Science Of Flaky Tests</h3>\n<p>A flaky test is one that fails to produce the same result each time the same analysis is run. The build will fail only occasionally: One time it will pass, another time fail, the next time pass again, without any changes to the build having been made.</p>\n<p>When I recall my testing nightmares, one case in particular comes into my mind. It was in a UI test. We built a custom-styled combo box (i.e. a selectable list with input field):</p>\n<p><img src=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/a43bbd70-005f-48f5-be31-a3e3342f0b9e/1-custom-select.png\" /></p>\n<p>With this combo box, you could search for a product and select one or more of the results. Many days, this test went fine, but at some point, things changed. In one of the approximately ten builds in our continuous integration (CI) system, the test for searching and selecting a product in this combo box failed.</p>\n<p>The screenshot of the fail shows the results list not being filtered, despite the search having been successful:</p>\n<p><img src=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/94451d92-e00d-4191-9a80-19cac209338a/2-flakiness-in-log.png\" /></p>\n<p>A flaky test like this <strong>can block the continuous deployment pipeline</strong>, making feature delivery slower than it needs to be. Moreover, a flaky test is problematic because it is not deterministic anymore — making it useless. After all, you wouldn’t trust one any more than you would trust a liar.</p>\n<p>In addition, flaky tests are <strong>expensive to repair</strong>, often requiring hours or even days to debug. Even though end-to-end tests are more prone to being flaky, I’ve experienced them in all kinds of tests: unit tests, functional tests, end-to-end tests, and everything in between.</p>\n<p>Another significant problem with flaky tests is the attitude they imbue in us developers. When I started working in test automation, I often heard developers say this in response to a failed test:</p>\n<blockquote>“Ahh, that build. Nevermind, just kick it off again. It will eventually pass, somewhen.”</blockquote>\n\n<p>This is a <strong>huge red flag for me</strong>. It shows me that the error in the build won’t be taken seriously. There is an assumption that a flaky test is not a real bug, but is “just” flaky, without needing to be taken care of or even debugged. The test will pass again later anyway, right? Nope! If such a commit is merged, in the worst case we will have a new flaky test in the product.</p>\n<h3>The Causes</h3>\n<p>So, flaky tests are problematic. What should we do about them? Well, if we know the problem, we can design a counter-strategy.</p>\n<p>I often encounter causes in everyday life. They can be <strong>found within the tests themselves</strong>. The tests might be suboptimally written, hold wrong assumptions, or contain bad practices. However, not only that. Flaky tests can be an indication of something far worse.</p>\n<p>In the following sections, we’ll go over the most common ones I have come across.</p>\n<h4>1. Test-Side Causes</h4>\n<p>In an ideal world, the initial state of your application should be pristine and 100% predictable. In reality, you never know whether the ID you’ve used in your test will always be the same.</p>\n<p>Let’s inspect two examples of a single fail on my part. Mistake number one was <strong>using an ID</strong> in my test fixtures:</p>\n<pre><code>{\n   \"id\": \"f1d2554b0ce847cd82f3ac9bd1c0dfca\",\n   \"name\": \"Variant product\",\n}\n</code></pre>\n\n<p>Mistake number two was searching for a <strong>unique selector</strong> to use in a UI test and thinking, “Ok, this ID seems unique. I’ll use it.”</p>\n<pre><code>&lt;!-- This is a text field I took from a project I worked on --&gt;\n&lt;input type=\"text\" id=\"sw-field--f1d2554b0ce847cd82f3ac9bd1c0dfca\" /&gt;\n</code></pre>\n\n<p>However, if I’d run the test on another installation or, later, on several builds in CI, then those tests might fail. Our application would generate the IDs anew, changing them between builds. So, the first possible cause is to be found in <strong>hardcoded IDs</strong>.</p>\n<p>The second cause can arise from randomly (or otherwise) <strong>generated demo data</strong>. Sure, you might be thinking that this “flaw” is justified — after all, the data generation is random — but think about debugging this data. It can be very difficult to see whether a bug is in the tests themselves or in the demo data.</p>\n<p>Next up is a test-side cause that I’ve struggled with numerous times: <strong>tests with cross-dependencies</strong>. Some tests may not be able to run independently or in a random order, which is problematic. In addition, previous tests could interfere with subsequent ones. These scenarios can cause flaky tests by introducing side effects.</p>\n<p>However, don’t forget that tests are about challenging <strong>assumptions</strong>. What happens if your assumptions are flawed to begin with? I’ve experienced these often, my favorite being flawed assumptions about time.</p>\n<p>One example is the usage of inaccurate waiting times, especially in UI tests — for example, by using <strong>fixed waiting times</strong>. The following line is taken from a <a href=\"https://nightwatchjs.org/\">Nightwatch.js</a> test.</p>\n<pre><code>// Please never do that unless you have a very good reason!\n// Waits for 1 second\nbrowser.pause(1000);\n</code></pre>\n\n<p>Another wrong assumption relates to time itself. I once discovered that a flaky <a href=\"https://phpunit.de/\">PHPUnit</a> test was failing only in our nightly builds. After some debugging, I found that the time shift between yesterday and today was the culprit. Another good example is failures because of <strong>time zones</strong>.</p>\n<p>False assumptions do not stop there. We can also have wrong assumptions about the <strong>order of data</strong>. Imagine a grid or list containing multiple entries with information, such as a list of currencies:</p>\n<p><img src=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b13311f9-965c-453f-88cf-0f6c9f737922/3-listing.png\" /></p>\n<p>We want to work with the information of the first entry, the “Czech koruna” currency. Can you be sure that your application will always place this piece of data as the first entry every time your test is executed? Could it be that the “Euro” or <strong>another currency</strong> will be the first entry on some occasions?</p>\n<p>Don’t assume that your data will come in the order you need it. Similar to hardcoded IDs, an order can change between builds, depending on the design of the application.</p>\n<h4>2. Environment-Side Causes</h4>\n<p>The next category of causes relates to everything outside of your tests. Specifically, we’re talking about the environment in which the tests are executed, the CI- and docker-related dependencies outside of your tests — all of those things you can barely influence, at least in your role as tester.</p>\n<p>A common environment-side cause is <strong>resource leaks</strong>: Often this would be an application under load, causing varying loading times or unexpected behavior. Large tests can easily cause leaks, eating up a lot of memory. Another common issue is the <strong>lack of cleanup</strong>.</p>\n<p>Incompatibility between dependencies gives me nightmares in particular. One nightmare occurred when I was working with Nightwatch.js for UI testing. Nightwatch.js uses WebDriver, which of course depends on Chrome. When Chrome sprinted ahead with an update, there was a problem with compatibility: Chrome, WebDriver, and Nightwatch.js itself no longer worked together, which caused our builds to fail from time to time.</p>\n<p><strong>Speaking of dependencies</strong>: <em>An honorable mention goes to any npm issues, such as missing permissions or npm being down. I experienced all of these in observing CI.</em></p>\n<p>When it comes to errors in UI tests due to environmental problems, keep in mind that you need the whole application stack in order for them to run. The more things that are involved, the more <strong>potential for error</strong>. JavaScript tests are, therefore, the most difficult tests to stabilize in web development, because they cover a large amount of code.</p>\n<h4>3. Product-Side Causes</h4>\n<p>Last but not least, we really have to be careful about this third area — an area with actual bugs. I’m talking about product-side causes of flakiness. One of the most well-known examples is the <strong>race conditions</strong> in an application. When this happens, the bug needs to be fixed in the product, not in the test! Trying to fix the test or the environment will have no use in this case.</p>\n<h3>Ways To Fight Flakiness</h3>\n<p>We have identified three causes of flakiness. We can build our counter-strategy on this! Of course, you will already have gained a lot by keeping the three causes in mind when you encounter flaky tests. You will already know what to look for and how to improve the tests. However, in addition to this, there are some strategies that will help us design, write, and debug tests, and we will look at them together in the following sections.</p>\n<h4>Focus On Your Team</h4>\n<p>Your team is arguably the <strong>most important factor</strong>. As a first step, admit that you have a problem with flaky tests. Getting the whole team’s commitment is crucial! Then, as a team, you need to decide how to deal with flaky tests.</p>\n<p>During the years I worked in technology, I came across four strategies used by teams to counter flakiness:</p>\n<ol>\n<li><strong>Do nothing and accept the flaky test result.</strong><br />Of course, this strategy is not a solution at all. The test will yield no value because you cannot trust it anymore — even if you accept the flakiness. So we can skip this one pretty quickly.</li>\n<li><strong>Retry the test until it passes.</strong><br />This strategy was common at the start of my career, resulting in the response I mentioned earlier. There was some acceptance with retrying tests until they passed. This strategy doesn’t require debugging, but it is lazy. In addition to hiding the symptoms of the problem, it will slow down your test suite even more, which makes the solution not viable. However, there might be some exceptions to this rule, which I’ll explain later.</li>\n<li><strong>Delete and forget about the test.</strong><br />This one is self-explanatory: Simply delete the flaky test, so that it doesn’t disturb your test suite anymore. Sure, it will save you money because you won’t need to debug and fix the test anymore. But it comes at the expense of losing a bit of test coverage and losing potential bug fixes. The test exists for a reason! Don’t shoot the messenger by deleting the test.</li>\n<li><strong>Quarantine and fix.</strong><br />I had the most success with this strategy. In this case, we would skip the test temporarily, and have the test suite constantly remind us that a test has been skipped. To make sure the fix doesn’t get overlooked, we would schedule a ticket for the next sprint. Bot reminders also work well. Once the issue causing the flakiness has been fixed, we’ll integrate (i.e. unskip) the test again. Unfortunately, we will lose coverage temporarily, but it will come back with a fix, so this will not take long.</li>\n</ol>\n<p><img src=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4ee80fe1-a3b0-4680-b7cd-2a23ef3e1e30/4-skipped-tests.png\" /></p>\n<p>These strategies help us deal with test problems at the workflow level, and I’m not the only one who has encountered them. In his article, Sam Saffron comes to the similar <a href=\"https://samsaffron.com/archive/2019/05/15/tests-that-sometimes-fail\">conclusion</a>. But in our day-to-day work, they help us to a limited extent. So, how do we proceed when such a task comes our way?</p>\n<h4>Keep Tests Isolated</h4>\n<p>When planning your test cases and structure, always keep your tests isolated from other tests, so that they’re able to be run in an independent or random order. The most important step is to <strong>restore a clean installation between tests</strong>. In addition, only test the workflow that you want to test, and create mock data only for the test itself. Another advantage of this shortcut is that it will <strong>improve test performance</strong>. If you follow these points, no side effects from other tests or leftover data will get in the way.</p>\n<p>The example below is taken from the UI tests of an e-commerce platform, and it deals with the customer’s login in the shop’s storefront. (The test is written in JavaScript, using the <a href=\"https://www.cypress.io/\">Cypress</a> framework.)</p>\n<pre><code>// File: customer-login.spec.js\nlet customer = {};\n\nbeforeEach(() =&gt; {\n    // Set application to clean state\n    cy.setInitialState()\n      .then(() =&gt; {\n        // Create test data for the test specifically\n        return cy.setFixture('customer');\n      })\n}):\n</code></pre>\n\n<p>The first step is resetting the application to a clean installation. It’s done as the first step in the <code>beforeEach</code> lifecycle hook to make sure that the reset is executed on every occasion. Afterwards, the test data is created specifically for the test — for this test case, a customer would be created via a custom command. Subsequently, we can start with the one workflow we want to test: the customer’s login.</p>\n<h4>Further Optimize The Test Structure</h4>\n<p>We can make some other small tweaks to make our test structure more stable. The first is quite simple: Start with smaller tests. As said before, the more you do in a test, the more can go wrong. <strong>Keep tests as simple as possible</strong>, and avoid a lot of logic in each one.</p>\n<p>When it comes to not assuming an order of data (for example, when dealing with the <strong>order of entries</strong> in a list in UI testing), we can design a test to function independent of any order. To bring back the example of the grid with information in it, we wouldn’t use pseudo-selectors or other CSS that have a strong dependency on order. Instead of the <code>nth-child(3)</code> selector, we could use text or other things for which order does not matter. For example, we could use an assertion like, “Find me the element with this one text string in this table”.</p>\n<h4>Wait! Test Retries Are Sometimes OK?</h4>\n<p>Retrying tests is a controversial topic, and rightfully so. I still think of it as an anti-pattern if the test is blindly retried until successful. However, there’s an important exception: When you can’t control errors, retrying can be a last resort (for example, to exclude errors from external dependencies). In this case, we cannot influence the source of the error. However, be extra careful when doing this: Don’t become blind to flakiness when retrying a test, and <strong>use notifications</strong> to remind you when a test is being skipped.</p>\n<p>The following example is one I used in our CI with GitLab. Other environments might have different syntax for achieving retries, but this should give you a taste:</p>\n<pre><code>test:\n    script: rspec\n    retry:\n        max: 2\n        when: runner_system_failure\n</code></pre>\n\n<p>In this example, we are configuring how many retries should be done if the job fails. What’s interesting is the possibility of retrying if there is an error in the runner system (for example, the job setup failed). We are <strong>choosing to retry</strong> our job only if something in the docker setup fails.</p>\n<p>Note that this will retry the whole job when triggered. If you wish to retry only the faulty test, then you’ll need to look for a feature in your test framework to support this. Below is an example from Cypress, which has supported retrying of a single test since version 5:</p>\n<pre><code>{\n    \"retries\": {\n        // Configure retry attempts for 'cypress run`\n        \"runMode\": 2,\n        // Configure retry attempts for 'cypress open`\n        \"openMode\": 2,\n    }\n}\n</code></pre>\n\n<p>You can activate test retries in Cypress’ configuration file, <code>cypress.json</code>. There, you can define the retry attempts in the test runner and headless mode.</p>\n<h4>Using Dynamic Waiting Times</h4>\n<p>This point is important for all kinds of tests, but especially UI testing. I can’t stress this enough: <strong>Don’t ever use fixed waiting times</strong> — at least not without a very good reason. If you do it, consider the possible outcomes. In the best case, you will choose waiting times that are too long, making the test suite slower than it needs to be. In the worst case, you won’t wait long enough, so the test won’t proceed because the application is not ready yet, causing the test to fail in a flaky manner. In my experience, this is the most common cause of flaky tests.</p>\n<p>Instead, use dynamic waiting times. There are many ways to do so, but Cypress handles them particularly well.</p>\n<p>All Cypress commands own an implicit waiting method: They already check whether the element that the command is being applied to exists in the DOM for the specified time — pointing to Cypress’ retry-ability. However, it only <strong>checks for existence</strong>, and nothing more. So I recommend going a step further — waiting for any changes in your website or application’s UI that a real user would also see, such as changes in the UI itself or in the animation.</p>\n<p><img src=\"https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/54292b15-188c-463b-a8f0-be204c5d07ba/5-waiting-times.png\" /></p>\n<p>This example uses an explicit waiting time on the element with the selector <code>.offcanvas</code>. The test would only proceed if the element is visible until the specified timeout, which you can configure:</p>\n<pre><code>// Wait for changes in UI (until element is visible)\ncy.get(#element).should('be.visible');\n</code></pre>\n\n<p>Another neat possibility in Cypress for dynamic waiting is its network features. Yes, we can wait for requests to occur and for the results of their responses. <strong>I use this kind of waiting especially often.</strong> In the example below, we define the request to wait for, use a <code>wait</code> command to wait for the response, and assert its status code:</p>\n<pre><code>// File: checkout-info.spec.js\n\n// Define request to wait for\ncy.intercept({\n    url: '/widgets/customer/info',\n    method: 'GET'\n}).as('checkoutAvailable');\n\n// Imagine other test steps here...\n\n// Assert the response’s status code of the request\ncy.wait('@checkoutAvailable').its('response.statusCode')\n  .should('equal', 200);\n</code></pre>\n\n<p>This way, we’re able to wait exactly as long as our application needs, making the tests more stable and less prone to flakiness due to resource leaks or other environmental issues.</p>\n<h4>Debugging Flaky Tests</h4>\n<p>We now know how to prevent flaky tests by design. But what if you’re already dealing with a flaky test? How can you get rid of it?</p>\n<p>When I was debugging, <strong>putting the flawed test in a loop</strong> helped me a lot in uncovering flakiness. For example, if you run a test 50 times, and it passes every time, then you can be more certain that the test is stable — maybe your fix worked. If not, you can at least get more insight into the flaky test.</p>\n<pre><code>// Use in build Lodash to repeat the test 100 times\nCypress._.times(100, (k) =&gt; {\n    it(`typing hello ${k + 1} / 100`, () =&gt; {\n        // Write your test steps in here\n    })\n})\n</code></pre>\n\n<p>Getting more insight into this flaky test is especially tough in CI. To get help, see whether your testing framework is able to get more information on your build. When it comes to front-end testing, you can usually make use of a <code>console.log</code> in your tests:</p>\n<pre><code>it('should be a Vue.JS component', () =&gt; {\n    // Mock component by a method defined before\n    const wrapper = createWrapper();\n\n\n    // Print out the component’s html\n    console.log(wrapper.html());\n\n    expect(wrapper.isVueInstance()).toBe(true);\n})\n</code></pre>\n\n<p>This example is taken from a <a href=\"https://jestjs.io/\">Jest</a> unit test in which I use a <code>console.log</code> to get the output of the HTML of the component being tested. If you use this logging possibility in Cypress’ test runner, you can even <strong>inspect the output</strong> in your developer tools of choice. In addition, when it comes to Cypress in CI, you can inspect this output in your CI’s log by using a plugin.</p>\n<p>Always look at the features of your test framework to get support with logging. In UI testing, most frameworks provide <strong>screenshot features</strong> — at least on a failure, a screenshot will be taken automatically. Some frameworks even provide <strong>video recording</strong>, which can be a huge help in getting insight into what is happening in your test.</p>\n<h3>Fight Flakiness Nightmares!</h3>\n<p>It’s important to continually hunt for flaky tests, whether by preventing them in the first place or by debugging and fixing them as soon as they occur. We need to take them seriously, because they can hint at problems in your application.</p>\n<h4>Spotting The Red Flags</h4>\n<p>Preventing flaky tests in the first place is best, of course. To quickly recap, here are some red flags:</p>\n<ul>\n<li>The test is large and contains a lot of logic.</li>\n<li>The test covers a lot of code (for example, in UI tests).</li>\n<li>The test makes use of fixed waiting times.</li>\n<li>The test depends on previous tests.</li>\n<li>The test asserts data that is not 100% predictable, such as the use of IDs, times, or demo data, especially randomly generated ones.</li>\n</ul>\n<p>If you keep the <strong>pointers and strategies</strong> from this article in mind, you can prevent flaky tests before they happen. And if they do come, you will know how to debug and fix them.</p>\n<p>These steps have really helped me regain confidence in our test suite. Our test suite seems to be stable at the moment. There could be issues in the future — nothing is 100% perfect. This knowledge and these strategies will help me to deal with them. Thus, I will grow confident in my ability to <strong>fight those flaky test nightmares</strong>.</p>\n<p>I hope I was able to relieve at least some of your pain and concerns about flakiness!</p>\n<h4>Further Reading</h4>\n<p>If you want to learn more on this topic, here are some neat resources and articles, which helped me a lot:</p>\n<ul>\n<li>Articles about “<a href=\"https://cypress.io/blog/tag/flake/\">flake</a>,” Cypress.io</li>\n<li>“<a href=\"https://www.cypress.io/blog/2020/09/25/guest-post-retrying-your-tests-is-actually-a-good-thing-if-your-approach-is-right/\">Retrying Your Tests Is Actually a Good Thing (If Your Approach Is Right)</a>,” Filip Hric, Cypress.io</li>\n<li>“<a href=\"https://engineering.atspotify.com/2019/11/18/test-flakiness-methods-for-identifying-and-dealing-with-flaky-tests/\">Test Flakiness: Methods for Identifying and Dealing With Flaky Tests</a>,” Jason Palmer, Spotify R&amp;D Engineering</li>\n<li>“<a href=\"https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html\">Flaky Tests at Google and How We Mitigate Them</a>,” John Micco, Google Testing Blog</li>\n</ul>","author":"","siteTitle":"Articles on Smashing Magazine — For Web Designers And Developers","siteHash":"ab069ca35bf300e9db0da36f49701f66485a5b0d2db0471dfeee07cef6204939","entryHash":"b2c4d1d9a1c5ce4e701bd3d9aa65f735d327d58192d3baa2e9989e0efe9d8586","category":"Tech"}