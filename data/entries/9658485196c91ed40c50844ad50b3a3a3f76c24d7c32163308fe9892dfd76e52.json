{"title":"Introducing Amazon Kinesis Data Analytics Studio – Quickly Interact with Streaming Data Using SQL, Python, or Scala","link":"https://aws.amazon.com/blogs/aws/introducing-amazon-kinesis-data-analytics-studio-quickly-interact-with-streaming-data-using-sql-python-or-scala/","date":1622132711000,"content":"<p>The best way to get timely insights and react quickly to new information you receive from your business and your applications is to analyze <a href=\"https://aws.amazon.com/streaming-data/\">streaming data</a>. This is data that must usually be processed sequentially and incrementally on a record-by-record basis or over sliding time windows, and can be used for a variety of analytics including correlations, aggregations, filtering, and sampling.</p> \n<p>To make it easier to analyze streaming data, today we are pleased to introduce <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">Amazon Kinesis Data Analytics Studio</a>.</p> \n<p>Now, from the <a href=\"https://console.aws.amazon.com/kinesis/home\">Amazon Kinesis console</a> you can select a <a href=\"https://aws.amazon.com/kinesis/data-streams/\">Kinesis data stream</a> and with a <strong>single click </strong>start a Kinesis Data Analytics Studio <strong>notebook</strong> powered by <a href=\"https://zeppelin.apache.org/\">Apache Zeppelin</a> and <a href=\"https://flink.apache.org/\">Apache Flink</a> to interactively analyze data in the stream. Similarly, you can select a cluster in the <a href=\"https://aws.amazon.com/msk/\">Amazon Managed Streaming for Apache Kafka</a> <a href=\"https://console.aws.amazon.com/msk/home\">console</a> to start a notebook to analyze data in <a href=\"https://kafka.apache.org/\">Apache Kafka</a> streams. You can also start a notebook from the <a href=\"https://console.aws.amazon.com/kinesisanalytics/home#/list/notebooks\">Kinesis Data Analytics Studio console</a> and connect to custom sources.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/KinesisAnalyticsStudio-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/KinesisAnalyticsStudio-1-1024x655.png\" /></a></p> \n<p>In the notebook, you can <strong>interact</strong> with streaming data and get results in seconds using SQL queries and Python or Scala programs. When you are satisfied with your results, with a few clicks you can promote your code to a <strong>production stream processing application</strong> that runs reliably at scale with no additional development effort.</p> \n<p>For new projects, we recommend that you use the new Kinesis Data Analytics Studio over <a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html\">Kinesis Data Analytics for SQL Applications</a>. Kinesis Data Analytics Studio combines ease of use with advanced analytical capabilities, which makes it possible to build sophisticated stream processing applications in minutes. Let’s see how that works in practice.</p> \n<p><span><strong>Using Kinesis Data Analytics Studio to Analyze Streaming Data<br /> </strong></span>I want to get a better understanding of the data sent by some sensors to a Kinesis data stream.</p> \n<p>To simulate the workload, I use this <code>random_data_generator.py</code> Python script. You don’t need to know Python to use Kinesis Data Analytics Studio. In fact, I am going to use SQL in the following steps. Also, you can avoid any coding and use the <a href=\"https://awslabs.github.io/amazon-kinesis-data-generator/\">Amazon Kinesis Data Generator</a> user interface (UI) to send test data to <a href=\"https://aws.amazon.com/kinesis/data-streams/\">Kinesis Data Streams</a> or <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">Kinesis Data Firehose</a>. I am using a Python script to have finer control over the data that is being sent.</p> \n<pre><code>import datetime\nimport json\nimport random\nimport boto3\n\nSTREAM_NAME = \"my-input-stream\"\n\n\ndef get_random_data():\n    current_temperature = round(10 + random.random() * 170, 2)\n    if current_temperature &gt; 160:\n        status = \"ERROR\"\n    elif current_temperature &gt; 140 or random.randrange(1, 100) &gt; 80:\n        status = random.choice([\"WARNING\",\"ERROR\"])\n    else:\n        status = \"OK\"\n    return {\n        'sensor_id': random.randrange(1, 100),\n        'current_temperature': current_temperature,\n        'status': status,\n        'event_time': datetime.datetime.now().isoformat()\n    }\n\n\ndef send_data(stream_name, kinesis_client):\n    while True:\n        data = get_random_data()\n        partition_key = str(data[\"sensor_id\"])\n        print(data)\n        kinesis_client.put_record(\n            StreamName=stream_name,\n            Data=json.dumps(data),\n            PartitionKey=partition_key)\n\n\nif __name__ == '__main__':\n    kinesis_client = boto3.client('kinesis')\n    send_data(STREAM_NAME, kinesis_client)</code></pre> \n<p>This script sends random records to my Kinesis data stream using JSON syntax. For example:</p> \n<div> \n <pre><code>{'sensor_id': 77, 'current_temperature': 93.11, 'status': 'OK', 'event_time': '2021-05-19T11:20:00.978328'}\n{'sensor_id': 47, 'current_temperature': 168.32, 'status': 'ERROR', 'event_time': '2021-05-19T11:20:01.110236'}\n{'sensor_id': 9, 'current_temperature': 140.93, 'status': 'WARNING', 'event_time': '2021-05-19T11:20:01.243881'}\n{'sensor_id': 27, 'current_temperature': 130.41, 'status': 'OK', 'event_time': '2021-05-19T11:20:01.371191'}\n</code></pre> \n</div> \n<p>From the <a href=\"https://awsc-integ.aws.amazon.com/kinesis/home#/streams/list\">Kinesis console</a>, I select a Kinesis data stream (<code>my-input-stream</code>) and choose <strong>Process data in real time</strong> from the <strong>Process</strong> drop-down. In this way, the stream is configured as a source for the notebook.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-data-streams.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-data-streams-1024x309.png\" /></a></p> \n<p>Then, in the following dialog box, I create an <strong>Apache Flink – Studio notebook</strong>.</p> \n<p>I enter a name (<code>my-notebook</code>) and a description for the notebook. The <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a> permissions to read from the Kinesis data stream I selected earlier (<code>my-input-stream</code>) are automatically attached to the IAM role assumed by the notebook.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-create.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-create-1024x787.png\" /></a></p> \n<p>I choose <strong>Create</strong> to open the <a href=\"https://console.aws.amazon.com/glue/home\">AWS Glue console</a> and create an empty database. Back in the Kinesis Data Analytics Studio console, I refresh the list and select the new database. It will define the metadata for my sources and destinations. From here, I can also review the default Studio notebook settings. Then, I choose <strong>Create Studio notebook</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-create-next.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-create-next-1024x873.png\" /></a></p> \n<p>Now that the notebook has been created, I choose <strong>Run</strong>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-run.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-run-1024x667.png\" /></a></p> \n<p>When the notebook is running, I choose <strong>Open in Apache Zeppelin</strong> to get access to the notebook and write code in SQL, Python, or Scala to interact with my streaming data and get insights in real time.</p> \n<p>In the notebook, I create a new note and call it <code>Sensors</code>. Then, I create a <code>sensor_data</code> table describing the format of the data in the stream:</p> \n<pre><code>%flink.ssql\n\nCREATE TABLE sensor_data (\n    sensor_id INTEGER,\n    current_temperature DOUBLE,\n    status VARCHAR(6),\n    event_time TIMESTAMP(3),\n    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n)\nPARTITIONED BY (sensor_id)\nWITH (\n    'connector' = 'kinesis',\n    'stream' = 'my-input-stream',\n    'aws.region' = 'us-east-1',\n    'scan.stream.initpos' = 'LATEST',\n    'format' = 'json',\n    'json.timestamp-format.standard' = 'ISO-8601'\n)\n</code></pre> \n<p>The first line in the previous command tells to Apache Zeppelin to provide a stream SQL environment (<code>%flink.ssql</code>) for the Apache Flink interpreter. I can also interact with the streaming data using a batch SQL environment (<code>%flink.bsql</code>), or Python (<code>%flink.pyflink</code>) or Scala (<code>%flink</code>) code.</p> \n<p>The first part of the <code>CREATE TABLE</code> statement is familiar to anyone who has used SQL with a database. A table is created to store the sensor data in the stream. The <code>WATERMARK</code> option is used to measure progress in the event time, as described in the <a href=\"https://ci.apache.org/projects/flink/flink-docs-master/docs/concepts/time/#event-time-and-watermarks\">Event Time and Watermarks section of the Apache Flink documentation</a>.</p> \n<p>The second part of the <code>CREATE TABLE</code> statement describes the connector used to receive data in the table (for example, <code>kinesis</code> or <code>kafka</code>), the name of the stream, the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Regions\">AWS Region</a>, the overall data format of the stream (such as <code>json</code> or <code>csv</code>), and the syntax used for timestamps (in this case, <a href=\"https://en.wikipedia.org/wiki/ISO_8601\">ISO 8601</a>). I can also choose the <a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_StartingPosition.html\">starting position</a> to process the stream, I am using <code>LATEST</code> to read the most recent data first.</p> \n<p>When the table is ready, I find it in the <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro\">AWS Glue Data Catalog</a> database I selected when I created the notebook:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-glue-catalog-1.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-glue-catalog-1-1024x171.png\" /></a></p> \n<p>Now I can run SQL queries on the <code>sensor_data</code> table and use sliding or tumbling windows to get a better understanding of what is happening with my sensors.</p> \n<p>For an overview of the data in the stream, I start with a simple <code>SELECT</code> to get all the content of the <code>sensor_data</code> table:</p> \n<pre><code>%flink.ssql(type=update)\n\nSELECT * FROM sensor_data;</code></pre> \n<p>This time the first line of the command has a parameter (<code>type=update</code>) so that the output of the <code>SELECT</code>, which is more than one row, is continuously updated when new data arrives.</p> \n<p>On the terminal of my laptop, I start the <code>random_data_generator.py</code> script:</p> \n<div> \n <pre><code>$ python3 random_data_generator.py</code></pre> \n</div> \n<p>At first I see a table that contains the data as it comes. To get a better understanding, I select a <strong>bar graph</strong> view. Then, I group the results by <code>status</code> to see their average <code>current_temperature</code>, as shown here:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-select-bar-graph-2.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-select-bar-graph-2-1024x709.png\" /></a></p> \n<p>As expected by the way I am generating these results, I have different average temperatures depending on the <code>status</code> (<code>OK</code>, <code>WARNING</code>, or <code>ERROR</code>). The higher the temperature, the greater the probability that something is not working correctly with my sensors.</p> \n<p>I can run the aggregated query explicitly using a SQL syntax. This time, I want the result computed on a sliding window of 1 minute with results updated every 10 seconds. To do so, I am using the <code>HOP</code> function in the <code>GROUP BY</code> section of the <code>SELECT</code> statement. To add the time to the output of the select, I use the <code>HOP_ROWTIME</code> function. For more information, see <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-agg/#group-window-aggregation\">how group window aggregations work in the Apache Flink documentation</a>.</p> \n<pre><code>%flink.ssql(type=update)\n\nSELECT sensor_data.status,\n       COUNT(*) AS num,\n       AVG(sensor_data.current_temperature) AS avg_current_temperature,\n       HOP_ROWTIME(event_time, INTERVAL '10' second, INTERVAL '1' minute) as hop_time\n  FROM sensor_data\n GROUP BY HOP(event_time, INTERVAL '10' second, INTERVAL '1' minute), sensor_data.status;</code></pre> \n<p>This time, I look at the results in table format:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-select-group-by.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-select-group-by-1024x466.png\" /></a></p> \n<p>To send the result of the query to a destination stream, I create a table and connect the table to the stream. First, I need to give permissions to the notebook to write into the stream.</p> \n<p>In the <a href=\"https://awsc-integ.aws.amazon.com/kinesisanalytics/home#/list/notebooks\">Kinesis Data Analytics Studio console</a>, I select <code>my-notebook</code>. Then, in the <strong>Studio notebooks details</strong> section, I choose <strong>Edit IAM permissions</strong>. Here, I can configure the sources and destinations used by the notebook and the IAM role permissions are updated automatically.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-details.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-details-1024x467.png\" /></a></p> \n<p>In the <strong>Included destinations in IAM policy</strong> section, I choose the destination and select <code>my-output-stream</code>. I save changes and wait for the notebook to be updated. I am now ready to use the destination stream.</p> \n<p>In the notebook, I create a <code>sensor_state</code> table connected to <code>my-output-stream</code>.</p> \n<pre><code>%flink.ssql\n\nCREATE TABLE sensor_state (\n    status VARCHAR(6),\n    num INTEGER,\n    avg_current_temperature DOUBLE,\n    hop_time TIMESTAMP(3)\n)\nWITH (\n'connector' = 'kinesis',\n'stream' = 'my-output-stream',\n'aws.region' = 'us-east-1',\n'scan.stream.initpos' = 'LATEST',\n'format' = 'json',\n'json.timestamp-format.standard' = 'ISO-8601');</code></pre> \n<p>I now use this <code>INSERT INTO</code> statement to continuously insert the result of the select into the <code>sensor_state</code> table.</p> \n<pre><code>%flink.ssql(type=update)\n\nINSERT INTO sensor_state\nSELECT sensor_data.status,\n    COUNT(*) AS num,\n    AVG(sensor_data.current_temperature) AS avg_current_temperature,\n    HOP_ROWTIME(event_time, INTERVAL '10' second, INTERVAL '1' minute) as hop_time\nFROM sensor_data\nGROUP BY HOP(event_time, INTERVAL '10' second, INTERVAL '1' minute), sensor_data.status;\n</code></pre> \n<p>The data is also sent to the destination Kinesis data stream (<code>my-output-stream</code>) so that it can be used by other applications. For example, the data in the destination stream can be used to update a real-time dashboard, or to monitor the behavior of my sensors after a software update.</p> \n<p>I am satisfied with the result. I want to deploy this query and its output as a Kinesis Analytics <strong>application</strong>.</p> \n<p>First, I create a <code>SensorsApp</code> note in my notebook and copy the statements that I want to execute as part of the application. The tables have already been created, so I just copy the INSERT INTO statement above.</p> \n<p>Then, from the menu at the top right of my notebook, I choose <strong>Build SensorsApp and export to Amazon S3</strong> and confirm the application name.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-build-export-s3.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-build-export-s3-1024x273.png\" /></a></p> \n<p>When the export is ready, I choose <strong>Deploy SensorsApp as Kinesis Analytics application</strong> in the same menu. After that, I fine-tune the configuration of the application. I set <a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/java/how-scaling.html#how-parallelism\">parallelism</a> to 1 because I have only one shard in my input Kinesis data stream and not a lot of traffic. Then, I run the application, without having to write any code.</p> \n<p>From the <a href=\"https://awsc-integ.aws.amazon.com/kinesisanalytics/home#/list/applications\">Kinesis Data Analytics applications console</a>, I choose <strong>Open Apache Flink dashboard</strong> to get more information about the execution of my application.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-apache-flink-console.png\"><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/05/19/kinesis-analytics-studio-apache-flink-console-1024x587.png\" /></a></p> \n<p><span><strong>Availability and Pricing<br /> </strong></span>You can use <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">Amazon Kinesis Data Analytics Studio</a> today in all <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Regions\">AWS Regions</a> where Kinesis Data Analytics is generally available. For more information, see the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\">AWS Regional Services List</a>.</p> \n<p>In Kinesis Data Analytics Studio, we run the <strong>open-source</strong> versions of <a href=\"https://zeppelin.apache.org/\">Apache Zeppelin</a> and <a href=\"https://flink.apache.org/\">Apache Flink</a>, and we contribute changes upstream. For example, we have contributed bug fixes for Apache Zeppelin, and we have contributed to AWS connectors for Apache Flink, such as those for Kinesis Data Streams and Kinesis Data Firehose. Also, we are working with the Apache Flink community to contribute availability improvements, including automatic classification of errors at runtime to understand whether errors are in user code or in application infrastructure.</p> \n<p>With Kinesis Data Analytics Studio, you pay based on the average number of Kinesis Processing Units (KPU) per hour, including those used by your running notebooks. One KPU comprises 1 vCPU of compute, 4 GB of memory, and associated networking. You also pay for running application storage and durable application storage. For more information, see the <a href=\"https://aws.amazon.com/kinesis/data-analytics/pricing/\">Kinesis Data Analytics pricing page</a>.</p> \n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/getting-started/\"><strong>Start using Kinesis Data Analytics Studio today to get better insights from your streaming data.</strong></a></p> \n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>","author":"Danilo Poccia","siteTitle":"AWS News Blog","siteHash":"6093e072e4117ec22616e844cb857d03ca62c57a411a8affc77cb5e8b6b15bf6","entryHash":"9658485196c91ed40c50844ad50b3a3a3f76c24d7c32163308fe9892dfd76e52","category":"Tech"}